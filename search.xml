<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>什么是 Shodan？</title>
      <link href="/2019/05/17/how-shodan/"/>
      <url>/2019/05/17/how-shodan/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是-Shodan？"><a href="#什么是-Shodan？" class="headerlink" title="什么是 Shodan？"></a>什么是 Shodan？</h2><p>首先，Shodan 是一个搜索引擎，但它与 Google 这种搜索网址的搜索引擎不同，Shodan 是用来搜索网络空间中在线设备的，你可以通过 Shodan 搜索指定的设备，或者搜索特定类型的设备，其中 Shodan 上最受欢迎的搜索内容是：webcam，linksys，cisco，netgear，SCADA等等。</p><p><strong>那么 Shodan 是怎么工作的呢？Shodan 通过扫描全网设备并抓取解析各个设备返回的 banner 信息，通过了解这些信息 Shodan 就能得知网络中哪一种 Web 服务器是最受欢迎的，或是网络中到底存在多少可匿名登录的 FTP 服务器。</strong></p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>这里就像是用 Google 一样，在主页的搜索框中输入想要搜索的内容即可，例如下面我搜索 “SSH”：</p><p><a href="http://image.3001.net/images/20161128/14803149659337.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803149659337.png!small" alt="27-1.png"></a></p><p>上图的搜索结果包含两个部分，左侧是大量的汇总数据包括：</p><ul><li>Results map – 搜索结果展示地图</li><li>Top services (Ports) – 使用最多的服务/端口</li><li>Top organizations (ISPs) – 使用最多的组织/ISP</li><li>Top operating systems – 使用最多的操作系统</li><li>Top products (Software name) – 使用最多的产品/软件名称</li></ul><p>随后，在中间的主页面我们可以看到包含如下的搜索结果：</p><ul><li>IP 地址</li><li>主机名</li><li>ISP</li><li>该条目的收录收录时间</li><li>该主机位于的国家</li><li>Banner 信息</li></ul><p>想要了解每个条目的具体信息，只需要点击每个条目下方的 details 按钮即可。此时，URL 会变成这种格式 <code>https://www.shodan.io/host/[IP]</code>，所以我们也可以通过直接访问指定的 IP 来查看详细信息。</p><p><a href="http://image.3001.net/images/20161128/14803149873250.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803149873250.png!small" alt="27-2.png"></a></p><p>上图中我们可以从顶部在地图中看到主机的物理地址，从左侧了解到主机的相关信息，右侧则包含目标主机的端口列表及其详细信息。</p><h3 id="使用搜索过滤"><a href="#使用搜索过滤" class="headerlink" title="使用搜索过滤"></a>使用搜索过滤</h3><p>如果像前面单纯只使用关键字直接进行搜索，搜索结果可能不尽人意，那么此时我们就需要使用一些特定的命令对搜索结果进行过滤，常见用的过滤命令如下所示：</p><ul><li><code>hostname</code>：搜索指定的主机或域名，例如 <code>hostname:&quot;google&quot;</code></li><li><code>port</code>：搜索指定的端口或服务，例如 <code>port:&quot;21&quot;</code></li><li><code>country</code>：搜索指定的国家，例如 <code>country:&quot;CN&quot;</code></li><li><code>city</code>：搜索指定的城市，例如 <code>city:&quot;Hefei&quot;</code></li><li><code>org</code>：搜索指定的组织或公司，例如 <code>org:&quot;google&quot;</code></li><li><code>isp</code>：搜索指定的ISP供应商，例如 <code>isp:&quot;China Telecom&quot;</code></li><li><code>product</code>：搜索指定的操作系统/软件/平台，例如 <code>product:&quot;Apache httpd&quot;</code></li><li><code>version</code>：搜索指定的软件版本，例如 <code>version:&quot;1.6.2&quot;</code></li><li><code>geo</code>：搜索指定的地理位置，参数为经纬度，例如 <code>geo:&quot;31.8639, 117.2808&quot;</code></li><li><code>before/after</code>：搜索指定收录时间前后的数据，格式为dd-mm-yy，例如 <code>before:&quot;11-11-15&quot;</code></li><li><code>net</code>：搜索指定的IP地址或子网，例如 <code>net:&quot;210.45.240.0/24&quot;</code></li></ul><h3 id="搜索实例"><a href="#搜索实例" class="headerlink" title="搜索实例"></a>搜索实例</h3><p>查找位于合肥的 Apache 服务器：</p><pre><code>apache city:&quot;Hefei&quot;</code></pre><p>查找位于国内的 Nginx 服务器：</p><pre><code>nginx country:&quot;CN&quot;</code></pre><p>查找 GWS(Google Web Server) 服务器：</p><pre><code>&quot;Server: gws&quot; hostname:&quot;google&quot;</code></pre><p>查找指定网段的华为设备：</p><pre><code>huawei net:&quot;61.191.146.0/24&quot;</code></pre><p>如上通过在基本关键字后增加指定的过滤关键字，能快速的帮助发现我们感兴趣的内容。当然，还有更快速更有意思的方法，那就是点击 Shodan 搜索栏右侧的 “Explore” 按钮，就会得到很多别人分享的搜索语法，你问我别人分享的语法有什么好玩的？那咱们就随便来看看吧：</p><p><a href="http://image.3001.net/images/20161128/14803150204500.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803150204500.png!small" alt="27-3.png"></a></p><p>咱们随便选取一个名为“NetSureveillance Web”的用户分享语法，从下面的描述信息我们基本就能得知这就是一个弱密码的漏洞，为了方便测试让我们把语法在增加一个国家的过滤信息，最终语法如下：</p><pre><code>Server: uc-httpd 1.0.0 200 OK Country:&quot;CN&quot;</code></pre><p><a href="http://image.3001.net/images/20161128/14803150355281.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803150355281.png!small" alt="27-4.png"></a></p><p>现在让我们随便选取一个页面进去输入，使用admin账号和空密码就能顺利进入了：）</p><p><a href="http://image.3001.net/images/20161128/1480315053949.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/1480315053949.png!small" alt="27-5.png"></a></p><h3 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能"></a>其他功能</h3><p>Shodan 不仅可以查找网络设备，它还具有其他相当不错的功能。</p><p>Exploits：每次查询完后，点击页面上的 “Exploits” 按钮，Shodan 就会帮我们查找针对不同平台、不同类型可利用的 exploits。当然也可以通过直接访问网址来自行搜索：<a href="https://exploits.shodan.io/welcome" target="_blank" rel="noopener">https://exploits.shodan.io/welcome</a>；</p><p><a href="http://image.3001.net/images/20161128/14803151039105.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803151039105.png!small" alt="27-10.png"></a></p><p>地图：每次查询完后，点击页面上的 “Maps” 按钮，Shodan 会将查询结果可视化的展示在地图当中；</p><p><a href="http://image.3001.net/images/20161128/14803151185705.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803151185705.png!small" alt="27-12.png"></a></p><p>报表：每次查询完后，点击页面上的 “Create Report” 按钮，Shodan 就会帮我们生成一份精美的报表，这是天天要写文档兄弟的一大好帮手啊；</p><p><a href="http://image.3001.net/images/20161128/14803151323230.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803151323230.png!small" alt="27-11.png"></a></p><h2 id="命令行下使用-Shodan"><a href="#命令行下使用-Shodan" class="headerlink" title="命令行下使用 Shodan"></a>命令行下使用 Shodan</h2><p><code>Shodan</code> 是由官方提供的 Python 库的，项目位于：<a href="https://github.com/achillean/shodan-python" target="_blank" rel="noopener">https://github.com/achillean/shodan-python</a></p><p><strong>安装</strong></p><pre><code>pip install shodan</code></pre><p>或者</p><pre><code>git clone https://github.com/achillean/shodan-python.git &amp;&amp; cd shodan-pythonpython setup.py install</code></pre><p>安装完后我们先看下帮助信息：</p><pre><code>➜  ~ shodan -hUsage: shodan [OPTIONS] COMMAND [ARGS]...Options:  -h, --help  Show this message and exit.Commands:  alert       Manage the network alerts for your account  # 管理账户的网络提示  convert     Convert the given input data file into a...  # 转换输入文件  count       Returns the number of results for a search  # 返回查询结果数量  download    Download search results and save them in a...  # 下载查询结果到文件  honeyscore  Check whether the IP is a honeypot or not.  # 检查 IP 是否为蜜罐  host        View all available information for an IP...  # 显示一个 IP 所有可用的详细信息  info        Shows general information about your account  # 显示账户的一般信息  init        Initialize the Shodan command-line  # 初始化命令行  myip        Print your external IP address  # 输出用户当前公网IP  parse       Extract information out of compressed JSON...  # 解析提取压缩的JSON信息，即使用download下载的数据  scan        Scan an IP/ netblock using Shodan.  # 使用 Shodan 扫描一个IP或者网段  search      Search the Shodan database  # 查询 Shodan 数据库  stats       Provide summary information about a search...  # 提供搜索结果的概要信息  stream      Stream data in real-time.  # 实时显示流数据</code></pre><h3 id="常用示例"><a href="#常用示例" class="headerlink" title="常用示例"></a>常用示例</h3><p><strong>init</strong></p><p>初始化命令行工具。</p><pre><code>➜  ~ shodan init [API_Key]Successfully initialized</code></pre><p><strong>count</strong> </p><p>返回查询的结果数量。</p><pre><code>➜  ~ shodan count microsoft iis 6.0575862</code></pre><p><strong>download</strong> </p><p>将搜索结果下载到一个文件中，文件中的每一行都是 JSON 格式存储的目标 banner 信息。默认情况下，该命令只会下载1000条结果，如果想下载更多结果需要增加 <code>--limit</code> 参数。</p><p><a href="http://image.3001.net/images/20161128/14803151703857.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803151703857.png!small" alt="27-6.png"></a></p><p><strong>parse</strong></p><p>我们可以使用 parse 来解析之前下载数据，它可以帮助我们过滤出自己感兴趣的内容，也可以用来将下载的数据格式从 JSON 转换成 CSV 等等其他格式，当然更可以用作传递给其他处理脚本的管道。例如，我们想将上面下载的数据以CSV格式输出IP地址、端口号和组织名称：</p><pre><code>➜  ~ shodan parse --fields ip_str,port,org --separator , microsoft-data.json.gz</code></pre><p><a href="http://image.3001.net/images/20161128/1480315189686.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/1480315189686.png!small" alt="27-7.png"></a></p><p><strong>host</strong></p><p>查看指定主机的相关信息，如地理位置信息，开放端口，甚至是否存在某些漏洞等信息。</p><p><a href="http://image.3001.net/images/20161128/14803152025933.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803152025933.png!small" alt="27-8.png"></a></p><p><strong>search</strong></p><p>直接将查询结果展示在命令行中，默认情况下只显示IP、端口号、主机名和HTTP数据。当然我们也可以通过使用 –fields 来自定义显示内容，例如，我们只显示IP、端口号、组织名称和主机名：</p><pre><code>➜  ~ shodan search --fields ip_str,port,org,hostnames microsoft iis 6.0</code></pre><p><a href="http://image.3001.net/images/20161128/14803152181807.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803152181807.png!small" alt="27-9.png"></a></p><h2 id="代码中使用-Shodan-库"><a href="#代码中使用-Shodan-库" class="headerlink" title="代码中使用 Shodan 库"></a>代码中使用 Shodan 库</h2><p>还是使用上一节讲到的 <a href="https://github.com/achillean/shodan-python" target="_blank" rel="noopener"><code>shodan</code></a> 库，安装方式这里不在阐述了。同样的，在使用 <code>shodan</code> 库之前需要初始化连接 API，代码如下：</p><pre><code>import shodanSHODAN_API_KEY = &quot;API_Key&quot;api = shodan.Shodan(SHODAN_API_KEY)</code></pre><p>随后，我们就可以搜索数据了，示例代码片如下：</p><pre><code>try:    # 搜索 Shodan    results = api.search(&#39;apache&#39;)    # 显示结果    print &#39;Results found: %s&#39; % results[&#39;total&#39;]    for result in results[&#39;matches&#39;]:            print result[&#39;ip_str&#39;]except shodan.APIError, e:    print &#39;Error: %s&#39; % e</code></pre><p><a href="http://image.3001.net/images/20161128/14803152351577.png" target="_blank" rel="noopener"><img src="http://image.3001.net/images/20161128/14803152351577.png!small" alt="27-13.png"></a></p><p>这里 <code>Shodan.search()</code> 会返回类似如下格式的 JSON 数据：</p><pre><code>{        &#39;total&#39;: 8669969,        &#39;matches&#39;: [                {                        &#39;data&#39;: &#39;HTTP/1.0 200 OK\r\nDate: Mon, 08 Nov 2010 05:09:59 GMT\r\nSer...&#39;,                        &#39;hostnames&#39;: [&#39;pl4t1n.de&#39;],                        &#39;ip&#39;: 3579573318,                        &#39;ip_str&#39;: &#39;89.110.147.239&#39;,                        &#39;os&#39;: &#39;FreeBSD 4.4&#39;,                        &#39;port&#39;: 80,                        &#39;timestamp&#39;: &#39;2014-01-15T05:49:56.283713&#39;                },                ...        ]}</code></pre><h3 id="常用-Shodan-库函数"><a href="#常用-Shodan-库函数" class="headerlink" title="常用 Shodan 库函数"></a>常用 Shodan 库函数</h3><ul><li><code>shodan.Shodan(key)</code> ：初始化连接API</li><li><code>Shodan.count(query, facets=None)</code>：返回查询结果数量</li><li><code>Shodan.host(ip, history=False)</code>：返回一个IP的详细信息</li><li><code>Shodan.ports()</code>：返回Shodan可查询的端口号</li><li><code>Shodan.protocols()</code>：返回Shodan可查询的协议</li><li><code>Shodan.services()</code>：返回Shodan可查询的服务</li><li><code>Shodan.queries(page=1, sort=&#39;timestamp&#39;, order=&#39;desc&#39;)</code>：查询其他用户分享的查询规则</li><li><code>Shodan.scan(ips, force=False)</code>：使用Shodan进行扫描，ips可以为字符或字典类型</li><li><code>Shodan.search(query, page=1, limit=None, offset=None, facets=None, minify=True)</code>：查询Shodan数据</li></ul>]]></content>
      
      
      <categories>
          
          <category> Shodan </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shodan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Launch Firefox with GeckoDriver (latest)</title>
      <link href="/2019/05/16/python-crawler-webdrive-firefox/"/>
      <url>/2019/05/16/python-crawler-webdrive-firefox/</url>
      
        <content type="html"><![CDATA[<h1 id="Launch-Firefox-with-GeckoDriver-latest"><a href="#Launch-Firefox-with-GeckoDriver-latest" class="headerlink" title="Launch Firefox with GeckoDriver (latest)"></a>Launch Firefox with GeckoDriver (latest)</h1><p>This article provides a detailed, step by step guide on how to launch  Firefox with Selenium Geckodriver. In this article we use the latest  versions of Selenium, Firefox &amp; Geckodriver and show you how you can  launch Firefox by providing updated code snippets. The tool versions  that we will be using in this article are –</p><ul><li><strong>Selenium</strong> – version 3.11.0</li><li><strong>Firefox</strong> – version 59.0.2 (Firefox Quantum)</li><li><strong>Geckodriver</strong> – version 0.20.1</li></ul><p><strong>Are you using an older version of Selenium Webdriver?</strong> Make sure you switch to the <a href="http://www.automationtestinghub.com/selenium-3/" target="_blank" rel="noopener"><strong>latest Selenium Webdriver version</strong></a> to avoid compatibility issues!!</p><p><img src="http://www.automationtestinghub.com/images/selenium/launch-firefox-with-geckodriver.png" alt="Launch Firefox with Selenium 3.0"></p><h2 id="What-is-Selenium-Geckodriver"><a href="#What-is-Selenium-Geckodriver" class="headerlink" title="What is Selenium Geckodriver?"></a>What is Selenium Geckodriver?</h2><p>Let us first start with the very basics – What is Gecko and  GeckoDriver? Gecko is a web browser engine used in many applications  developed by Mozilla Foundation and the Mozilla Corporation, most  noticeably the Firefox web browser, its mobile version other than iOS  devices, their email client Thunderbird and many other open source  software projects. You can get more information about Gecko here – <a href="https://en.wikipedia.org/wiki/Gecko_(software)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gecko_(software)</a></p><p><strong>Geckodriver is a proxy for using W3C WebDriver-compatible  clients to interact with Gecko-based browsers i.e. Mozilla Firefox in  this case.</strong> This program provides the HTTP API described by the  WebDriver protocol to communicate with Gecko browsers. It translates  calls into the Marionette automation protocol by acting as a proxy  between the local and remote ends.</p><h2 id="How-things-worked-before-Geckodriver-and-Selenium-3"><a href="#How-things-worked-before-Geckodriver-and-Selenium-3" class="headerlink" title="How things worked before Geckodriver and Selenium 3"></a>How things worked before Geckodriver and Selenium 3</h2><p>If you are new to Selenium and you have started directly with  Selenium 3.x, you would not know how Firefox was launched with the  previous versions of Selenium (version 2.53 and before). It was a pretty  straight forward process where you were not required to use Geckodriver  or any other driver. After you <a href="http://www.automationtestinghub.com/download-and-install-selenium/" target="_blank" rel="noopener">download and install Selenium</a>, you just write the code to instantiate the WebDriver and open Firefox. The code snippet is shown below –</p><p>public class FirefoxTest {          public static void main(String[] args) {         WebDriver driver = new FirefoxDriver();         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>1234567</th><th>public class FirefoxTest {        public static void main(String[] args) {        WebDriver driver = new FirefoxDriver();        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p>If you just run this code, you would notice that Firefox browser would get opened and <a href="http://Google.com" target="_blank" rel="noopener">Google.com</a>  would be displayed in the browser. This is how it worked with Selenium  2.53 and before. Let’s see whats the new implementation in Selenium 3.</p><h2 id="What-happens-when-you-don’t-use-Firefox-Geckodriver-with-Selenium-3-x"><a href="#What-happens-when-you-don’t-use-Firefox-Geckodriver-with-Selenium-3-x" class="headerlink" title="What happens when you don’t use Firefox Geckodriver with Selenium 3.x"></a>What happens when you don’t use Firefox Geckodriver with Selenium 3.x</h2><p>To try this out, all that you need to do is point your JAR files to  the latest version of Selenium 3 and then run the same code that is  given above. You will now notice that <a href="http://Google.com" target="_blank" rel="noopener">Google.com</a> page would not open in a new Firefox window. Instead you will see an error message as shown below –</p><blockquote><p><strong>java.lang.IllegalStateException: The path to the  driver executable must be set by the webdriver.gecko.driver system  property; for more information, see <a href="https://github.com/mozilla/geckodriver" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver</a>. The latest version can be downloaded from <a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver/releases</a></strong></p></blockquote><p><img src="http://www.automationtestinghub.com/images/selenium/webdriver-gecko-driver-error.png" alt="Geckodriver Error"></p><p>You will need to use Selenium Geckodriver to remove this error. Let us see how this can be done.</p><h2 id="How-to-use-Selenium-Geckodriver-to-launch-Firefox"><a href="#How-to-use-Selenium-Geckodriver-to-launch-Firefox" class="headerlink" title="How to use Selenium Geckodriver to launch Firefox"></a>How to use Selenium Geckodriver to launch Firefox</h2><p>To launch Firefox with Selenium Geckodriver, you will first need to  download Geckodriver and then set its path. This can be done in two ways  as depicted in the below image – </p><p><img src="http://www.automationtestinghub.com/images/selenium/process-to-use-geckodriver.png" alt="Process to use Geckodriver"></p><h2 id="Check-if-Firefox-is-32-bit-or-64-bit"><a href="#Check-if-Firefox-is-32-bit-or-64-bit" class="headerlink" title="Check if Firefox is 32-bit or 64-bit"></a>Check if Firefox is 32-bit or 64-bit</h2><p><strong>There are two versions of Geckodriver for Windows: 32-bit and 64-bit</strong>.  Based on whether your Firefox is 32-bit or 64-bit, you need to download  the corresponding Geckodriver exe. In this section, you will first  check whether your Firefox is 32-bit or 64-bit</p><p><strong>1.</strong> Open Firefox on your machine. Click on Hamburger icon from the right corner to open the menu as shown below</p><p><img src="http://www.automationtestinghub.com/images/selenium/firefox-hamburger-icon-menu.png" alt="Open Firefox Menu"></p><p><strong>2.</strong> From this menu, click on Help icon (Help icon is marked in red box in the above image)</p><p><strong>3.</strong> Once you click on Help icon, the <strong>Help Menu</strong> would be displayed</p><p><img src="http://www.automationtestinghub.com/images/selenium/firefox-help-menu.png" alt="Help Menu - Firefox"></p><p><strong>4.</strong> Click on <strong>About Firefox</strong> from the Help menu. <strong>About Mozilla Firefox</strong> popup would be displayed</p><p><img src="http://www.automationtestinghub.com/images/selenium/about-mozilla-firefox-check-firefox-version-59.png" alt="Check if Mozilla Firefox is 32-bit or 64-bit"></p><p><strong>5.</strong> Note down whether Firefox is 32 or 64 bit. <strong>For us, Firefox is 64-bit as shown in the above image.</strong> Now close this popup and close Firefox as well.</p><h2 id="Download-the-latest-version-of-Selenium-Geckodriver"><a href="#Download-the-latest-version-of-Selenium-Geckodriver" class="headerlink" title="Download the latest version of Selenium Geckodriver"></a>Download the latest version of Selenium Geckodriver</h2><p>Follow the steps given below to download Geckodriver –</p><p><strong>1.</strong> Open this Github page – <a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver/releases</a></p><p><strong>2.</strong> Download the latest release (windows version) based on whether your Firefox is 32-bit or 64-bit. We are downloading <strong>geckodriver-v0.20.1-win64.zip</strong>, as we have 64-bit Firefox</p><p><img src="http://www.automationtestinghub.com/images/selenium/download-geckodriver-latest-version.png" alt="Download latest version of GeckoDriver"></p><p><strong>3.</strong> Once the zip file is downloaded, unzip it to retrieve the driver – geckodriver.exe</p><p>This completes the downloading process. Now let’s see how you can use  it in your project. There are 2 methods using which you can configure  this driver in your project. You can use any of these methods.</p><p>According to this <a href="http://gs.statcounter.com/browser-market-share" target="_blank" rel="noopener">statcounter report</a>, Chrome is by far the most used browser. If you are learning Selenium, <a href="http://www.automationtestinghub.com/selenium-chromedriver/" target="_blank" rel="noopener"><strong>make sure that you run your scripts on Chrome browser as well</strong></a></p><h2 id="Launch-Firefox-Method-1-webdriver-gecko-driver-system-property"><a href="#Launch-Firefox-Method-1-webdriver-gecko-driver-system-property" class="headerlink" title="Launch Firefox Method 1 : webdriver.gecko.driver system property"></a>Launch Firefox Method 1 : webdriver.gecko.driver system property</h2><p>With this method, you will have to add an additional line of code in  your test case. Follow the steps given below to use this method – </p><p><strong>1.</strong> Copy the entire path where you unzipped geckodriver.exe.  Let us assume that the location is – D:\Firefox\geckodriver.exe. You  will need to add <strong>System.setProperty</strong> with the driver location to your code.</p><p>The code to launch Firefox browser would look like this – </p><p><strong>Important Note 1:</strong> In the folder paths in the below  code, we have used double backslash (\). This is because Java treats  single back slash () as an escape character. So you would need to use  double back slash, everywhere you add some folder path.</p><p>public class FirefoxTest {          public static void main(String[] args) {                 System.setProperty(“webdriver.gecko.driver”,”D:\Firefox\geckodriver.exe”);          WebDriver driver = new FirefoxDriver();         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>123456789</th><th>public class FirefoxTest {        public static void main(String[] args) {                System.setProperty(“webdriver.gecko.driver”,”D:\Firefox\geckodriver.exe”);         WebDriver driver = new FirefoxDriver();        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p><strong>Important Note 2:</strong> If you are using older versions of  Geckodriver (v0.16.1 or before), then you will also need to provide the  Firefox Binary, otherwise you might get the below error – </p><p><em>org.openqa.selenium.SessionNotCreatedException: Expected browser  binary location, but unable to find binary in default location, no  ‘moz:firefoxOptions.binary’ capability provided, and no binary flag set  on the command line</em></p><p><strong>But please note that this is needed only for Geckodriver v0.16.1 or before.</strong>  So for older Gecko versions, please use the below code where Firefox  binary location has been provided using FirefoxOptions class.</p><p>public class FirefoxTest {          public static void main(String[] args) {                 System.setProperty(“webdriver.gecko.driver”,”D:\Firefox\geckodriver.exe”);                  FirefoxOptions options = new FirefoxOptions();         options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //This is the location where you have installed Firefox on your machine          WebDriver driver = new FirefoxDriver(options);         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>123456789101112</th><th>public class FirefoxTest {        public static void main(String[] args) {                System.setProperty(“webdriver.gecko.driver”,”D:\Firefox\geckodriver.exe”);                FirefoxOptions options = new FirefoxOptions();        options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //This is the location where you have installed Firefox on your machine         WebDriver driver = new FirefoxDriver(options);        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p><strong>3.</strong> Run this code to verify that everything is working fine. You will notice that <a href="http://google.com" target="_blank" rel="noopener">google.com</a> gets opened in new Firefox window</p><h3 id="Launch-Firefox-Method-2-Set-property-in-Environment-Variables"><a href="#Launch-Firefox-Method-2-Set-property-in-Environment-Variables" class="headerlink" title="Launch Firefox Method 2 : Set property in Environment Variables"></a>Launch Firefox Method 2 : Set property in Environment Variables</h3><p><strong>1.</strong> Copy the entire folder location where geckodriver.exe is  saved. If the entire path is D:\Firefox\geckodriver.exe, then the folder  location would be D:\Firefox\</p><p><strong>2.</strong> Open Advanced tab in System Properties window as shown in below image.</p><p><img src="http://www.automationtestinghub.com/images/selenium/system-properties.png" alt="System Properties"></p><p><strong>3.</strong> Open Environment Variables window. </p><p><img src="http://www.automationtestinghub.com/images/selenium/environment-variables-section.png" alt="Environment Variables Window"></p><p><strong>4.</strong> In System variables section, select the Path variable  (highlighted in the above image) and click on Edit button. Then add the  location of Geckodriver that we copied in step 1 (D:\Firefox), to path  variable (below image shows UI for Windows 10)</p><p><img src="http://www.automationtestinghub.com/images/selenium/add-geckodriver-path-to-environment-variables.png" alt="Add GeckoDriver path to Environment Variables"></p><p><strong>5.</strong> If you are using Windows 7, then move to the end of the  Variable value field, then add a semi-colon (;) and then add the folder  location as shown below (Semicolon acts as a separator between multiple  values in the field)</p><p><img src="http://www.automationtestinghub.com/images/selenium/add-geckodriver-location-in-path-variable.png" alt="Add GeckoDriver in Path Variable - Windows 7"></p><p><strong>6.</strong> Click on Ok button to close the windows. Once the path is  set, you would not need to set the System property every time in the  test script. Your test script would simply look like this – </p><p><strong>For GeckoDriver v0.20, v0.19.0, v0.18.0 and v0.17.0 –</strong> </p><p>public class FirefoxTest {          public static void main(String[] args) {         WebDriver driver = new FirefoxDriver();         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>1234567</th><th>public class FirefoxTest {        public static void main(String[] args) {        WebDriver driver = new FirefoxDriver();        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p> <strong>For GeckoDriver v0.16.1 or before –</strong> </p><p>public class FirefoxTest {          public static void main(String[] args) {         FirefoxOptions options = new FirefoxOptions();         options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //This is the location where you have installed Firefox on your machine          WebDriver driver = new FirefoxDriver(options);         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>12345678910</th><th>public class FirefoxTest {        public static void main(String[] args) {        FirefoxOptions options = new FirefoxOptions();        options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //This is the location where you have installed Firefox on your machine         WebDriver driver = new FirefoxDriver(options);        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p><strong>7.</strong> Run the code to check that it works fine.</p><p> This completes our article on how you can use launch Firefox with  Selenium GeckoDriver. Try it out and let us know if this worked for you.  <strong>Feel free to contact us using comments section if you face any issue while implementing this.</strong></p><p> <strong>UPDATE 1 [30 April, 2017]:</strong> Use DesiredCapabilities and FirefoxOptions to launch Firefox with Selenium GeckoDriver</p><p>public class FirefoxTest {          public static void main(String[] args) {         FirefoxOptions options = new FirefoxOptions();         options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //Location where Firefox is installed          DesiredCapabilities capabilities = DesiredCapabilities.firefox();         capabilities.setCapability(“moz:firefoxOptions”, options);         //set more capabilities as per your requirements          FirefoxDriver driver = new FirefoxDriver(capabilities);         driver.get(“<a href="http://www.google.com&quot;)" target="_blank" rel="noopener">http://www.google.com&quot;)</a>;     } }</p><table><thead><tr><th>1234567891011121314</th><th>public class FirefoxTest {        public static void main(String[] args) {        FirefoxOptions options = new FirefoxOptions();        options.setBinary(“C:\Program Files (x86)\Mozilla Firefox\firefox.exe”); //Location where Firefox is installed         DesiredCapabilities capabilities = DesiredCapabilities.firefox();        capabilities.setCapability(“moz:firefoxOptions”, options);        //set more capabilities as per your requirements         FirefoxDriver driver = new FirefoxDriver(capabilities);        driver.get(“<a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a>“);    }}</th></tr></thead><tbody><tr><td></td></tr></tbody></table><p><strong>Here are a few hand-picked articles for you to read next:</strong></p><ul><li><a href="http://www.automationtestinghub.com/selenium-headless-chrome-firefox/" target="_blank" rel="noopener">Learn how to launch firefox in headless mode with Selenium</a></li><li><a href="http://www.automationtestinghub.com/disable-firefox-logs-selenium/" target="_blank" rel="noopener">Disable low level console logs when you run your tests on Firefox</a></li><li><a href="http://www.automationtestinghub.com/cucumber-selenium-testing-tutorial/" target="_blank" rel="noopener">Add the power of Cucumber BDD to your test scripts</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Firefox </tag>
            
            <tag> Webdriver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之五：正则表达式</title>
      <link href="/2019/05/15/python-crawler-regular-expression-5/"/>
      <url>/2019/05/15/python-crawler-regular-expression-5/</url>
      
        <content type="html"><![CDATA[<h2 id="正文："><a href="#正文：" class="headerlink" title="正文："></a>正文：</h2><p>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。</p><h2 id="通过一个小实例来了解正则表达式的作用"><a href="#通过一个小实例来了解正则表达式的作用" class="headerlink" title="通过一个小实例来了解正则表达式的作用"></a>通过一个小实例来了解正则表达式的作用</h2><pre><code># 从字符串 str 中找出abc/数字import res = &#39;123abc456eabc789&#39;re.findall(r&#39;abc&#39;, s)# result: [&#39;abc&#39;, &#39;abc&#39;]re.findall(&#39;[0-9]+&#39;,s)# result: [&#39;123&#39;, &#39;456&#39;, &#39;789&#39;]</code></pre><h2 id="限定符"><a href="#限定符" class="headerlink" title="限定符"></a>限定符</h2><p>限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-ae6d73e71049b6bb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>5-1.jpg</p><pre><code>import res = &#39;Chapter1 Chapter2 Chapter10 Chapter99 fheh&#39;re.findall(&#39;Chapter[1-9][0-9]*&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]+&#39;, s)# result: [&#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]?&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]{0,1}&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]{1,2}&#39;, s)# result:[&#39;Chapter10&#39;, &#39;Chapter99&#39;]</code></pre><h2 id="贪婪匹配与非贪婪匹配"><a href="#贪婪匹配与非贪婪匹配" class="headerlink" title="贪婪匹配与非贪婪匹配"></a>贪婪匹配与非贪婪匹配</h2><p>正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符</p><pre><code>import re# 贪婪模式s = &#39;&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;&#39;re.findall(&#39;&lt;.*&gt;&#39;, s)# result: [&#39;&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;&#39;]# 非贪婪模式re.findall(&#39;&lt;.*?&gt;&#39;, s)# result: [&#39;&lt;H1&gt;&#39;, &#39;&lt;/H1&gt;&#39;]</code></pre><h2 id="定位符"><a href="#定位符" class="headerlink" title="定位符"></a>定位符</h2><p>定位符使您能够将正则表达式固定到行首或行尾。它们还使您能够创建这样的正则表达式，这些正则表达式出现在一个单词内、在一个单词的开头或者一个单词的结尾。</p><p>定位符用来描述字符串或单词的边界，^ 和 $ 分别指字符串的开始与结束，\b 描述单词的前或后边界，\B 表示非单词边界。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-53ebbb5379727d24.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>5-2.jpg</p><pre><code>import res = &#39;Chapter1 Chapter2 Chapter11 Chapter99&#39;re.findall(&#39;^Chapter[1-9][0-9]{0,1}&#39;, s)# result: [&#39;Chapter1&#39;]re.findall(&#39;^Chapter[1-9][0-9]{0,1}$&#39;, &#39;Chapter99&#39;)# result: [&#39;Chapter99&#39;]re.findall(r&#39;\bCha&#39;, &#39; Chapter&#39;)# result: [&#39;Cha&#39;]re.findall(r&#39;ter\b&#39;, &#39; Chapter&#39;)# result: [&#39;ter&#39;]re.findall(r&#39;\Bapt&#39;, &#39;Chapter&#39;)# result: [&#39;apt&#39;]re.findall(r&#39;\Bapt&#39;, &#39;aptitude&#39;)# result: []</code></pre><h2 id="分组与捕获组"><a href="#分组与捕获组" class="headerlink" title="分组与捕获组"></a>分组与捕获组</h2><p>要说明白捕获，就要先从分组开始。重复单字符我们可以使用限定符，如果重复字符串，用什么呢？ 对！用小括号，小括号里包裹指定字表达式（子串），这就是分组。之后就可以限定这个子表示的重复次数了。</p><p>那么，什么是捕获呢？使用小括号指定一个子表达式后，匹配这个子表达式的文本（即匹配的内容）可以在表达式或者其他过程中接着用，怎么用呢？至少应该有个指针啥的引用它吧？ 对！默认情况下，每个分组（小括号）会自动拥有一个组号,从左到右，以分组的左括号为标志，第一个出现的分组组号为1，后续递增。如果出现嵌套，</p><pre><code>s = &#39;aaa111aaa , bbb222 , 333ccc&#39;re.findall(r&#39;[a-z]+(\d+)[a-z]&#39;, s)# result:[&#39;111&#39;]re.findall(r&#39;[a-z]+\d+[a-z]&#39;, s)  # 对比(\d+)的用法# result:[&#39;aaa111a&#39;]re.findall(r&#39;[a-z]+\d+[a-z]+&#39;, s)  # 对比(\d+)的用法# result:[&#39;aaa111aaa&#39;]s = &#39;111aaa222aaa111 , 333bbb444bb33&#39;re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, s)# result:[(&#39;111&#39;, &#39;aaa&#39;, &#39;222&#39;, &#39;aaa&#39;, &#39;111&#39;)]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, &#39;333bbb444bb33&#39;)# result:[]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, &#39;333bbb444bbb333&#39;)# result:[(&#39;333&#39;, &#39;bbb&#39;, &#39;444&#39;, &#39;bbb&#39;, &#39;333&#39;)]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\1)(\2)&#39;, &#39;333bbb444bbb333&#39;)# result:[]</code></pre><h2 id="非捕获组"><a href="#非捕获组" class="headerlink" title="非捕获组"></a>非捕获组</h2><p>用圆括号将所有选择项括起来，相邻的选择项之间用|分隔。但用圆括号会有一个副作用，使相关的匹配会被缓存。可用?:放在第一个选项前来消除这种副作用。</p><p>其中 ?: 是非捕获元之一，还有两个非捕获元是 ?= 和 ?!，这两个还有更多的含义，前者为正向预查，在任何开始匹配圆括号内的正则表达式模式的位置来匹配搜索字符串，后者为负向预查，在任何开始不匹配该正则表达式模式的位置来匹配搜索字符串。</p><pre><code># (?:pattern)与(pattern)不同之处只是在于不捕获结果，非捕获组只匹配结果，但不捕获结果，也不会分配组号s = &#39;industry is industries lala industyyy industiii&#39;re.findall(r&#39;industr(?:y|ies)&#39;, s)# result: [&#39;industry&#39;, &#39;industries&#39;]s = &#39;Windows2000 Windows3.1&#39;re.findall(r&#39;Windows(?=95|98|NT|2000)&#39;, s)# result: [&#39;Windows&#39;]# 匹配 &quot;Windows2000&quot; 中的 &quot;Windows&quot;,不匹配 &quot;Windows3.1&quot; 中的 &quot;Windows&quot;。s = &#39;Windows2000 Windows3.1&#39;re.findall(r&#39;Windows(?!95|98|NT|2000)&#39;, s)# result: [&#39;Windows&#39;]# 匹配 &quot;Windows3.1&quot; 中的 &quot;Windows&quot;,不匹配 &quot;Windows2000&quot; 中的 &quot;Windows&quot;。s = &#39;aaa111aaa,bbb222,333ccc,444ddd444,555eee666,fff777ggg&#39;re.findall(r&#39;([a-z]+)\d+([a-z]+)&#39;, s)# result:[(&#39;aaa&#39;, &#39;aaa&#39;), (&#39;fff&#39;, &#39;ggg&#39;)]re.findall(r&#39;(?P&lt;g1&gt;[a-z]+)\d+(?P=g1)&#39;, s)# result:[&#39;aaa&#39;]re.findall(r&#39;(?P&lt;g1&gt;[a-z]+)\d+(?P=g1)&#39;, &#39;aaa111aaa,bbb222,333ccc,444ddd444,555eee666,fff777fff&#39;)# result:[&#39;aaa&#39;, &#39;fff&#39;]re.findall(r&#39;[a-z]+(\d+)([a-z]+)&#39;, s)# result: [(&#39;111&#39;, &#39;aaa&#39;), (&#39;777&#39;, &#39;ggg&#39;)]re.findall(r&#39;([a-z]+)\d+&#39;, s)# result:[&#39;aaa&#39;, &#39;bbb&#39;, &#39;ddd&#39;, &#39;eee&#39;, &#39;fff&#39;]re.findall(r&#39;([a-z]+)\d+\1&#39;, s)# result:[&#39;aaa&#39;]s = &#39;I have a dog , I have a cat&#39;re.findall(r&#39;I have a (?:dog|cat)&#39;, s)# result: [&#39;I have a dog&#39;, &#39;I have a cat&#39;]re.findall(r&#39;I have a dog|cat&#39;, s)# result: [&#39;I have a dog&#39;, &#39;cat&#39;]s = &#39;ababab abbabb aabaab abbbbbab&#39;re.findall(r&#39;\b(?:ab)+\b&#39;, s)# result: [&#39;ababab&#39;]re.findall(r&#39;\b(ab)+\b&#39;, s)# result: [&#39;ab&#39;]</code></pre><h2 id="反向引用"><a href="#反向引用" class="headerlink" title="反向引用"></a>反向引用</h2><p>捕获组(Expression)在匹配成功时，会将子表达式匹配到的内容，保存到内存中一个以数字编号的组里，可以简单的认为是对一个局部变量进行了赋值，这时就可以通过反向引用方式，引用这个局部变量的值。一个捕获组(Expression)在匹配成功之前，它的内容可以是不确定的，一旦匹配成功，它的内容就确定了，反向引用的内容也就是确定的了。</p><p>反向引用必然要与捕获组一同使用的，如果没有捕获组，而使用了反向引用的语法，不同语言的处理方式不一致，有的语言会抛异常，有的语言会当作普通的转义处理。</p><pre><code>s = &#39;Is is the cost of of gasoline going up up ?&#39;re.findall(r&#39;\b([a-z]+) \1\b&#39;, s, re.I)  # 大小写不敏感# result: [&#39;Is&#39;, &#39;of&#39;, &#39;up&#39;]re.findall(r&#39;\b([a-z]+) \1\b&#39;, s)# result: [&#39;of&#39;, &#39;up&#39;]s = &#39;http://www.w3cschool.cc:80/html/html-tutorial.html&#39;re.findall(r&#39;(\w+):\/\/([^/:]+)(:\d*)?([^# ]*)&#39;, s)# result: [(&#39;http&#39;, &#39;www.w3cschool.cc&#39;, &#39;:80&#39;, &#39;/html/html-tutorial.html&#39;)]</code></pre><h2 id="注意区别：pattern-、pattern-、-pattern-、-pattern"><a href="#注意区别：pattern-、pattern-、-pattern-、-pattern" class="headerlink" title="注意区别：pattern+?、pattern*?、(?!pattern)、(?:pattern)"></a>注意区别：pattern+?、pattern*?、(?!pattern)、(?:pattern)</h2><h4 id="pattern-、pattern"><a href="#pattern-、pattern" class="headerlink" title="pattern+?、pattern*?"></a>pattern+?、pattern*?</h4><p>这两个比较常用，表示懒惰匹配，即匹配符合条件的尽量短的字符串。默认情况下 + 和 * 是贪婪匹配，即匹配尽可能长的字符串，在它们后面加上 ? 表示想要进行懒惰匹配。</p><h4 id="pattern"><a href="#pattern" class="headerlink" title="(?!pattern)"></a>(?!pattern)</h4><p>表示一个过滤条件，若字符串符合 pattern 则将其过滤掉。在分析日志时很有用，例如想过滤掉包含 info 标记的日志可以写 ^(?!.<em>info).</em>$。</p><h4 id="pattern-1"><a href="#pattern-1" class="headerlink" title="(?:pattern)"></a>(?:pattern)</h4><p>这条规则主要是为了优化性能，对匹配没有影响。它表示括号内的子表达式匹配的结果不需要返回也不会被 <img src="https://math.jianshu.com/math?formula=1" alt="1">2 之类的反向引用。</p><h2 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h2><ol><li>import re导入正则表达式模块。Python中所有正则表达式的函数都在re模块中，所以我们要先引入re模块；</li><li>用re.compile()函数创建一个Regex对象（参数就是要匹配的内容的正则表达式）；</li><li>用Regex对象的search()方法来查找一段字符串，返回那个匹配的对象num，num中是一段相应的描述信息；</li><li>调用匹配对象num的group()方法，返回实际匹配文本的字符串。</li></ol><pre><code>import res1 = &quot;once upon a time&quot;s2 = &quot;There once was a man from NewYork&quot;print(re.findall(r&#39;^once&#39;, s1))# result: [&#39;once&#39;]print(re.findall(r&#39;^once&#39;, s2))# result: []print(re.findall(r&#39;time$&#39;, s1))# result: [&#39;time&#39;]print(re.findall(r&#39;times$&#39;, s1))# result: []print(re.findall(r&#39;^time$&#39;, s1))# result: []print(re.findall(r&#39;^time$&#39;, &#39;time&#39;))# result: [&#39;time&#39;]## compiles = &#39;111,222,aaa,bbb,ccc333,444ddd&#39;rule = r&#39;\b\d+\b&#39;compiled_rule = re.compile(rule)print(compiled_rule.findall(s))# result: [&#39;111&#39;, &#39;222&#39;]## matchprint(re.match(&#39;www&#39;, &#39;www.runoob.com&#39;).span())  # 在起始位置匹配# result: (0, 3)print(re.match(&#39;com&#39;, &#39;www.runoob.com&#39;))  # 不在起始位置匹配# result: Noneline = &quot;Cats are smarter than dogs&quot;matchObj = re.match(r&#39;(.*) are (.*?) .*&#39;, line, re.M | re.I)if matchObj:    print(&quot;matchObj.group() : &quot;, matchObj.group())    # result: matchObj.group() :  Cats are smarter than dogs    print(&quot;matchObj.group(1) : &quot;, matchObj.group(1))    # result: matchObj.group(1) :  Cats    print(&quot;matchObj.group(2) : &quot;, matchObj.group(2))    # result: matchObj.group(2) :  smarterelse:    print(&quot;No match!!&quot;)## searchprint(re.search(&#39;www&#39;, &#39;www.runoob.com&#39;).span())  # 在起始位置匹配# result: (0, 3)print(re.search(&#39;com&#39;, &#39;www.runoob.com&#39;).span())  # 不在起始位置匹配# result: (11, 14)line = &quot;Cats are smarter than dogs&quot;;searchObj = re.search(r&#39;(.*) are (.*?) .*&#39;, line, re.M | re.I)if searchObj:    print(&quot;searchObj.group() : &quot;, searchObj.group())    # result: searchObj.group() :  Cats are smarter than dogs    print(&quot;searchObj.group(1) : &quot;, searchObj.group(1))    # result: searchObj.group(1) :  Cats    print(&quot;searchObj.group(2) : &quot;, searchObj.group(2))    # result: searchObj.group(2) :  smarterelse:    print(&quot;Nothing found!!&quot;)## match与searchline = &quot;Cats are smarter than dogs&quot;matchObj = re.match(r&#39;dogs&#39;, line, re.M | re.I)if matchObj:    print(&quot;match --&gt; matchObj.group() : &quot;, matchObj.group())else:    print(&quot;No match!!&quot;)    # result: No match!!matchObj = re.search(r&#39;dogs&#39;, line, re.M | re.I)if matchObj:    print(&quot;search --&gt; matchObj.group() : &quot;, matchObj.group())    # result: search --&gt; matchObj.group() :  dogselse:    print(&quot;No match!!&quot;)## subphone = &quot;2004-959-559 # This is Phone Number&quot;num = re.sub(r&#39;#.*$&#39;, &quot;&quot;, phone)print(&quot;Phone Num : &quot;, num)# result: Phone Num :  2004-959-559num = re.sub(r&#39;\D&#39;, &quot;&quot;, phone)print(&quot;Phone Num : &quot;, num)# result: Phone Num :  2004959559</code></pre><h2 id="正则表达式与BeautifulSoup"><a href="#正则表达式与BeautifulSoup" class="headerlink" title="正则表达式与BeautifulSoup"></a>正则表达式与BeautifulSoup</h2><pre><code>from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObj = BeautifulSoup(html, &#39;lxml&#39;)# print(bsObj.prettify())images = bsObj.findAll(&quot;img&quot;, {&quot;src&quot;: re.compile(&quot;\.\.\/img\/gifts/img.*\.jpg&quot;)})for image in images:    print(image[&quot;src&quot;])# result: ../img/gifts/img1.jpg# ../img/gifts/img2.jpg# ../img/gifts/img3.jpg# ../img/gifts/img4.jpg# ../img/gifts/img6.jpg</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> regex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之四：BeautifulSoup</title>
      <link href="/2019/05/14/python-crawler-beautifulsoup-4/"/>
      <url>/2019/05/14/python-crawler-beautifulsoup-4/</url>
      
        <content type="html"><![CDATA[<h2 id="正文："><a href="#正文：" class="headerlink" title="正文："></a>正文：</h2><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.</p><p>安装: pip install beautifulsoup4</p><blockquote><p>名字是beautifulsoup 的包,是 Beautiful Soup3 的发布版本,因为很多项目还在使用BS3, 所以 beautifulsoup 包依然有效.但是如果你在编写新项目,那么你应该安装的 beautifulsoup4，这个包兼容Python2和Python3</p></blockquote><h2 id="BeautifulSoup的基础使用"><a href="#BeautifulSoup的基础使用" class="headerlink" title="BeautifulSoup的基础使用"></a>BeautifulSoup的基础使用</h2><p>下面的一段HTML代码将作为例子被多次用到.这是《爱丽丝梦游仙境》的的一段内容(以后简称文档)</p><pre><code>html_doc = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;</code></pre><p>使用BeautifulSoup解析这段代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出</p><pre><code>from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, &quot;lxml&quot;)print(soup.prettify())# result:# &lt;html&gt;#  &lt;head&gt;#   &lt;title&gt;#    The Dormouse&#39;s story#   &lt;/title&gt;#  &lt;/head&gt;#  &lt;body&gt;#   &lt;p class=&quot;title&quot;&gt;#    &lt;b&gt;#     The Dormouse&#39;s story#    &lt;/b&gt;#   &lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;#    Once upon a time there were three little sisters; and their names were#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;#     Elsie#    &lt;/a&gt;#    ,#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;#     Lacie#    &lt;/a&gt;#    and#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;#     Tillie#    &lt;/a&gt;#    ;# and they lived at the bottom of a well.#   &lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;#    ...#   &lt;/p&gt;#  &lt;/body&gt;# &lt;/html&gt;</code></pre><p>几个简单的浏览结构化数据的方法</p><pre><code>soup.title# result: &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;soup.title.name# result: &#39;title&#39;soup.title.string# result: &#39;The Dormouse&#39;s story&#39;soup.title.text# result: &#39;The Dormouse&#39;s story&#39;soup.title.parent.name# result: &#39;head&#39;soup.p# result: &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;soup.p[&#39;class&#39;]# result: &#39;title&#39;soup.a# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;soup.find_all(&#39;a&#39;)#  result:# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]soup.find(id=&quot;link3&quot;)# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre><p>从文档中找到所有<a>标签的链接</a></p><pre><code>for link in soup.find_all(&#39;a&#39;):    print(link.get(&#39;href&#39;))# result:# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie</code></pre><p>从文档中获取所有文字内容</p><pre><code>print(soup.get_text())# result:# The Dormouse&#39;s story## The Dormouse&#39;s story## Once upon a time there were three little sisters; and their names were# Elsie,# Lacie and# Tillie;# and they lived at the bottom of a well.## ...</code></pre><p>从文档中解析导航树</p><pre><code># 直接子节点print(soup.body.contents)for child in soup.descendants:    print(child)# 所有后代节点for child in soup.descendants:    print(child)# 节点内容1-包含换行符for string in soup.strings:    print(repr(string))# 节点内容2-不包含换行符for string in soup.stripped_strings:    print(repr(string))# 兄弟节点(没有一个兄弟节点会返回None)print(soup.p.next_sibling)print(soup.p.previous_sibling)for sibling in soup.a.next_siblings:    print(repr(sibling))# 前后节点print(soup.head.next_element)print(soup.head.previous_element)# 父节点from urllib.request import urlopenhtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObj = BeautifulSoup(html)print(bsObj.find(&quot;img&quot;, {&quot;src&quot;: &quot;../img/gifts/img1.jpg&quot;}).parent.previous_sibling.get_text())</code></pre><p>从文档中解析CSS选择器</p><pre><code>print(soup.select(&#39;title&#39;))print(soup.select(&#39;a&#39;))print(soup.select(&#39;b&#39;))print(soup.select(&#39;.sisiter&#39;))print(soup.select(&#39;#link1&#39;))print(soup.select(&#39;p #link1&#39;))print(soup.select(&#39;head &gt; title&#39;))print(soup.select(&#39;a[class=&quot;sister&quot;]&#39;))print(soup.select(&#39;a[href=&quot;http://example.com/elsie&quot;]&#39;))print(soup.select(&#39;p a[href=&quot;http://example.com/elsie&quot;]&#39;))</code></pre><p>结合正则表达式或其他逻辑条件</p><pre><code>import refor tag in soup.find_all(href=re.compile(&quot;elsie&quot;)):    print(tag)# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;for tag in soup.find_all(&quot;a&quot;, class_=&quot;sister&quot;):    print(tag)# result：# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;for tag in soup.find_all([&quot;a&quot;, &quot;b&quot;]):    print(tag)# result :# &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;for tag in soup.find_all(True):    print(tag)# result:# &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;# &lt;body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;# &lt;/body&gt;&lt;/html&gt;# &lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;# &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;# &lt;body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;# &lt;/body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;def has_class_but_no_id(tag):    return tag.has_attr(&#39;class&#39;) and not tag.has_attr(&#39;id&#39;)for tag in soup.find_all(has_class_but_no_id):    print(tag)# result:# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</code></pre><h2 id="BeautifulSoup的主要参数的使用"><a href="#BeautifulSoup的主要参数的使用" class="headerlink" title="BeautifulSoup的主要参数的使用"></a>BeautifulSoup的主要参数的使用</h2><pre><code>html_doc = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;import bs4from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, &quot;lxml&quot;)# text参数soup.find_all(text=&quot;Elsie&quot;)soup.find_all(text=[&quot;Elsie&quot;, &quot;Lacie&quot;, &quot;Tillie&quot;])soup.find_all(text=re.compile(&quot;Dormouse&quot;))# limit参数soup.find_all(&quot;a&quot;, limit=2)# recursive参数soup.html.find_all(&quot;title&quot;)soup.html.find_all(&quot;title&quot;, recursive=False)# tag对象print(soup.title)print(soup.head)print(soup.a)print(soup.p)print(type(soup.a))print(soup.name)print(soup.head.name)print(soup.p.attrs)print(soup.p[&#39;class&#39;])print(soup.p.get(&#39;class&#39;))soup.p[&#39;class&#39;] = &quot;newclass&quot;print(soup.p)del soup.p[&#39;class&#39;]print(soup.p)# NavigableString对象print(soup.p.string)print(type(soup.p.string))# BeautifulSoup对象print(type(soup.name))print(soup.name)print(soup.attrs)# Comment对象print(soup.a)print(soup.a.string)print(type(soup.a.string))if type(soup.a.string) == bs4.element.Comment:    print(soup.a.string)</code></pre><h2 id="BeautifulSoup解析在线网页实例"><a href="#BeautifulSoup解析在线网页实例" class="headerlink" title="BeautifulSoup解析在线网页实例"></a>BeautifulSoup解析在线网页实例</h2><pre><code>from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/exercises/exercise1.html&#39;)bsObj = BeautifulSoup(html.read(), &#39;lxml&#39;)print(bsObj.h1)# CSS属性from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/pages/warandpeace.html&#39;)bsObj = BeautifulSoup(html, &#39;lxml&#39;)nameList = bsObj.findAll(&#39;span&#39;, {&#39;class&#39;: &#39;green&#39;})for name in nameList:    print(name.get_text())# find()和findall()from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/pages/warandpeace.html&#39;)bsObj = BeautifulSoup(html)allText = bsObj.findAll(id=&#39;text&#39;)print(allText[0].get_text())</code></pre><p>更多使用方法参见<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">BeautifulSoup 中文文档</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之三：基本工具库urllib和requests</title>
      <link href="/2019/05/13/python-crawler-urllib-requests-3/"/>
      <url>/2019/05/13/python-crawler-urllib-requests-3/</url>
      
        <content type="html"><![CDATA[<h1 id="Python网络爬虫实战之三：基本工具库urllib和requests"><a href="#Python网络爬虫实战之三：基本工具库urllib和requests" class="headerlink" title="Python网络爬虫实战之三：基本工具库urllib和requests"></a>Python网络爬虫实战之三：基本工具库urllib和requests</h1><h1 id="一、urllib"><a href="#一、urllib" class="headerlink" title="一、urllib"></a>一、urllib</h1><h3 id="urllib简介"><a href="#urllib简介" class="headerlink" title="urllib简介"></a>urllib简介</h3><p>urllib是Python中一个功能强大用于操作URL，并在爬虫时经常用到的一个基础库，无需额外安装，默认已经安装到python中。</p><h3 id="urllib在python2-x与python3-x中的区别"><a href="#urllib在python2-x与python3-x中的区别" class="headerlink" title="urllib在python2.x与python3.x中的区别"></a>urllib在python2.x与python3.x中的区别</h3><p>在python2.x中，urllib分为urllib和urllib2，在python3.x中合并为urllib。两者使用起来不太一样，注意转换。</p><table><thead><tr><th>Python2.x</th><th>Python3.x</th></tr></thead><tbody><tr><td>import urllib2</td><td>import urllib.request，urllib.error</td></tr><tr><td>import urllib</td><td>import urllib.request，urllib.error，urllib.parse</td></tr><tr><td>import urlparse</td><td>import urllib.parse</td></tr><tr><td>import urlopen</td><td>import urllib.request.urlopen</td></tr><tr><td>import urlencode</td><td>import urllib.parse.urlencode</td></tr><tr><td>import urllib.quote</td><td>import urllib.request.quote</td></tr><tr><td>cookielib.CookieJar</td><td>http.CookieJar</td></tr><tr><td>urllib2.Request</td><td>urllib.request.Request</td></tr></tbody></table><h3 id="urllib的四个子模块"><a href="#urllib的四个子模块" class="headerlink" title="urllib的四个子模块"></a>urllib的四个子模块</h3><p>Python3.6.0中urllib模块包括一下四个子模块，urllib模块是一个运用于URL的包（urllib is a package that collects several modules for working with URLs）</p><ul><li>urllib.request用于访问和读取URLS（urllib.request for opening and reading URLs），就像在浏览器里输入网址然后回车一样，只需要给这个库方法传入URL和其他参数就可以模拟实现这个过程。</li><li>urllib.error包括了所有urllib.request导致的异常（urllib.error containing the exceptions raised by urllib.request），我们可以捕捉这些异常，然后进行重试或者其他操作以确保程序不会意外终止。</li><li>urllib.parse用于解析URLS（urllib.parse for parsing URLs），提供了很多URL处理方法，比如拆分、解析、合并、编码。</li><li>urllib.robotparser用于解析robots.txt文件（urllib.robotparser for parsing robots.txt files），然后判断哪些网站可以爬，哪些网站不可以爬。</li></ul><h3 id="使用urllib打开网页"><a href="#使用urllib打开网页" class="headerlink" title="使用urllib打开网页"></a>使用urllib打开网页</h3><p>最基本的方法打开网页</p><pre><code># 最基本的方法打开网页from urllib.request import urlopenresponse = urlopen(&quot;http://www.baidu.com&quot;)print(type(response))print(response.status)print(response.getheaders())print(response.getheader(&#39;Server&#39;))html = response.read()print(html)</code></pre><p>携带data参数打开网页</p><pre><code># 携带data参数打开网页from urllib.parse import urlencodefrom urllib.request import urlopendata = bytes(urlencode({&#39;word&#39;: &#39;hello&#39;}), encoding=&#39;utf8&#39;)response = urlopen(&#39;http://httpbin.org/post&#39;, data=data)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>携带timeout参数打开网页1</p><pre><code>#  携带timeout参数打开网页1from urllib.request import urlopen# response = urllib.request.urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=1)print(response.read())</code></pre><p>携带timeout参数打开网页2</p><pre><code># 携带timeout参数打开网页2from urllib.request import urlopentry:    response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)    print(response.read())except Exception as e:    print(e)</code></pre><p>通过构建Request打开网页1</p><pre><code># 通过构建Request打开网页1from urllib.request import Requestfrom urllib.request import urlopenrequest = Request(&#39;https://python.org&#39;)response = urlopen(request)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页2</p><pre><code># 通过构建Request打开网页2from urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;headers = {    &#39;User-Agent&#39;: &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;,    &#39;Host&#39;: &#39;httpbin.org&#39;}dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, headers=headers, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>与通过构建Request打开网页2对比</p><pre><code># 与通过构建Request打开网页2对比from urllib.request import Requestfrom urllib.request import urlopenreq = Request(url=url, data=data, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页3：通过add_header方</p><pre><code># 通过构建Request打开网页3：通过add_header方法添加headersfrom urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, method=&#39;POST&#39;)req.add_header(&#39;User-Agent&#39;, &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>urlencode()的使用</p><pre><code># urlencode()的使用from urllib.parse import urlencodefrom urllib.request import urlopendata = {&#39;first&#39;: &#39;true&#39;, &#39;pn&#39;: 1, &#39;kd&#39;: &#39;Python&#39;}data = urlencode(data).encode(&#39;utf-8&#39;)datapage = urlopen(req, data=data).read()page</code></pre><p>使用代理打开网页</p><pre><code># 使用代理from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy_handler = ProxyHandler({&#39;http&#39;: &#39;106.56.102.140:8070&#39;})opener = build_opener(proxy_handler)try:    response = opener.open(&#39;http://www.baidu.com/&#39;)    print(response.read().decode(&#39;utf-8&#39;))except URLError as e:    print(e.reason)</code></pre><h1 id="二、requests"><a href="#二、requests" class="headerlink" title="二、requests"></a>二、requests</h1><p>相比较urllib模块，requests模块要简单很多，但是需要单独安装：</p><ul><li>在windows系统下只需要在命令行输入命令 pip install requests 即可安装。</li><li>在 linux 系统下，只需要输入命令 sudo pip install requests ，即可安装。</li></ul><p>requests库的八个主要方法</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>requests.request()</td><td>构造一个请求，支持以下各种方法</td></tr><tr><td>requests.get()</td><td>向html网页提交get请求的方法</td></tr><tr><td>requests.post()</td><td>向html网页提交post请求的方法</td></tr><tr><td>requests.head()</td><td>获取html头部信息的主要方法</td></tr><tr><td>requests.put()</td><td>向html网页提交put请求的方法</td></tr><tr><td>requests.options()</td><td>向html网页提交options请求的方法</td></tr><tr><td>requests.patch()</td><td>向html网页提交局部修改的请求</td></tr><tr><td>requests.delete()</td><td>向html网页提交删除的请求</td></tr></tbody></table><p>请求之后，服务器通过response返回数据，response具体参数如下图：</p><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>r.status_code</td><td>http请求的返回状态，若为200则表示请求成功</td></tr><tr><td>r.text</td><td>http响应内容的字符串形式，即返回的页面内容</td></tr><tr><td>r.encoding</td><td>从http header 中猜测的相应内容编码方式</td></tr><tr><td>r.apparent_encoding</td><td>从内容中分析出的响应内容编码方式（备选编码方式）</td></tr><tr><td>r.content</td><td>http响应内容的二进制形式</td></tr></tbody></table><h3 id="requests-request-method-url-kwargs"><a href="#requests-request-method-url-kwargs" class="headerlink" title="requests.request(method, url, **kwargs)"></a>requests.request(method, url, **kwargs)</h3><ul><li>method：即 get、post、head、put、options、patch、delete</li><li>url：即请求的网址</li><li>**kwargs：控制访问的参数，具体参数如下：</li><li><ul><li>params：字典或字节序列，作为参数增加到url中。使用这个参数可以把一些键值对以?key1=value1&amp;key2=value2的模式增加到url中</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 paramsimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>data：字典，字节序或文件对象，重点作为向服务器提供或提交资源是提交，作为request的内容，与params不同的是，data提交的数据并不放在url链接里， 而是放在url链接对应位置的地方作为数据来存储。它也可以接受一个字符串对象。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 dataimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, data=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;23&quot;, #     &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>json：json格式的数据， json合适在相关的html，http相关的web开发中非常常见， 也是http最经常使用的数据格式， 他是作为内容部分可以向服务器提交。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 jsonimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, json=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;{\&quot;key1\&quot;: \&quot;value1\&quot;, \&quot;key2\&quot;: \&quot;value2\&quot;}&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;36&quot;, #     &quot;Content-Type&quot;: &quot;application/json&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>files：字典， 是用来向服务器传输文件时使用的字段。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 fileimport requests# filejiatao.txt 文件的内容是文本“www.baidu.com www.cctvjiatao.com”files = {&#39;file&#39;: open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;rb&quot;)}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, files=files)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {#     &quot;file&quot;: &quot;www.baidu.com www.cctvjiatao.com&quot;#   }, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;182&quot;, #     &quot;Content-Type&quot;: &quot;multipart/form-data; boundary=ee12ea6a4fd2b8a3318566775f2b268f&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>headers：字典是http的相关语，对应了向某个url访问时所发起的http的头字段， 可以用这个字段来定义http的访问的http头，可以用来模拟任何我们想模拟的浏览器来对url发起访问。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 headersimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload, headers=headers)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>cookies：字典或CookieJar，指的是从http中解析cookie</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 cookiesimport requestscookies = dict(cookies_are=&#39;working&#39;)r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/cookies&#39;, cookies=cookies)print(r.url)# result: http://httpbin.org/cookiesprint(r.text)# result:# {#   &quot;cookies&quot;: {#     &quot;cookies_are&quot;: &quot;working&quot;#   }# }</code></pre><ul><li><ul><li>auth：元组，用来支持http认证功能</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 authimport requestscs_user = &#39;用户名&#39;cs_psw = &#39;密码&#39;r = requests.request(&#39;GET&#39;, &#39;https://api.github.com&#39;, auth=(cs_user, cs_psw))print(r.url)# result: 待补充print(r.text)# result: 待补充</code></pre><ul><li><ul><li>timeout: 用于设定超时时间， 单位为秒，当发起一个get请求时可以设置一个timeout时间， 如果在timeout时间内请求内容没有返回， 将产生一个timeout的异常。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 timeoutimport requestsr = requests.request(&#39;GET&#39;, &#39;http://github.com&#39;, timeout=0.001)print(r.url)# result: 报错 socket.timeout: timed out</code></pre><ul><li><ul><li>proxies：字典， 用来设置访问代理服务器。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 proxiesimport requestsproxies = {    &#39;https&#39;: &#39;http://41.118.132.69:4433&#39;}# 也可以通过环境变量设置代理# export HTTP_PROXY=&#39;http://10.10.1.10:3128&#39;# export HTTPS_PROXY=&#39;http://10.10.1.10:1080&#39;r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, proxies=proxies)print(r.url)# result: http://httpbin.org/getprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get&quot;# }</code></pre><ul><li><ul><li>verify：开关， 用于认证SSL证书， 默认为True</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 verify，SSL证书验证import requestsr = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=True)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=False)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://github.com&#39;, verify=True)print(r.text)</code></pre><ul><li><ul><li>allow_redirects: 开关， 表示是否允许对url进行重定向， 默认为True。</li></ul></li><li><ul><li>stream: 开关， 指是否对获取内容进行立即下载， 默认为True。</li></ul></li><li><ul><li>cert： 用于设置保存本地SSL证书路径</li></ul></li></ul><h3 id="requests-get-url-params-None-kwargs"><a href="#requests-get-url-params-None-kwargs" class="headerlink" title="requests.get(url, params=None, **kwargs)"></a>requests.get(url, params=None, **kwargs)</h3><pre><code># 官方文档def get(url, params=None, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;get&#39;, url, params=params, **kwargs)</code></pre><h3 id="requests-post-url-data-None-json-None-kwargs"><a href="#requests-post-url-data-None-json-None-kwargs" class="headerlink" title="requests.post(url, data=None, json=None, **kwargs)"></a>requests.post(url, data=None, json=None, **kwargs)</h3><pre><code># 官方文档def post(url, data=None, json=None, **kwargs):    return request(&#39;post&#39;, url, data=data, json=json, **kwargs)</code></pre><h3 id="requests-head-url-kwargs"><a href="#requests-head-url-kwargs" class="headerlink" title="requests.head(url, **kwargs)"></a>requests.head(url, **kwargs)</h3><pre><code># 官方文档def head(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, False)    return request(&#39;head&#39;, url, **kwargs)</code></pre><h3 id="requests-options-url-kwargs"><a href="#requests-options-url-kwargs" class="headerlink" title="requests.options(url, **kwargs)"></a>requests.options(url, **kwargs)</h3><pre><code># 官方文档def options(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;options&#39;, url, **kwargs)</code></pre><h3 id="requests-put-url-data-None-kwargs"><a href="#requests-put-url-data-None-kwargs" class="headerlink" title="requests.put(url, data=None, **kwargs)"></a>requests.put(url, data=None, **kwargs)</h3><pre><code># 官方文档def put(url, data=None, **kwargs):    return request(&#39;put&#39;, url, data=data, **kwargs)</code></pre><h3 id="requests-patch-url-data-None-kwargs"><a href="#requests-patch-url-data-None-kwargs" class="headerlink" title="requests.patch(url, data=None, **kwargs)"></a>requests.patch(url, data=None, **kwargs)</h3><pre><code># 官方文档def patch(url, data=None, **kwargs):    return request(&#39;patch&#39;, url, data=data, **kwargs)</code></pre><blockquote><p>requests.patch和request.put类似。<br> 两者不同的是： 当我们用patch时仅需要提交需要修改的字段。<br> 而用put时，必须将所有字段一起提交到url，未提交字段将会被删除。<br> patch的好处是：节省网络带宽。</p></blockquote><h3 id="requests-delete-url-kwargs"><a href="#requests-delete-url-kwargs" class="headerlink" title="requests.delete(url, **kwargs)"></a>requests.delete(url, **kwargs)</h3><pre><code># 官方文档def delete(url, **kwargs):    return request(&#39;delete&#39;, url, **kwargs)</code></pre><h3 id="requests库的异常"><a href="#requests库的异常" class="headerlink" title="requests库的异常"></a>requests库的异常</h3><p>注意requests库有时会产生异常，比如网络连接错误、http错误异常、重定向异常、请求url超时异常等等。所以我们需要判断r.status_codes是否是200，在这里我们怎么样去捕捉异常呢？<br> 这里我们可以利用r.raise_for_status() 语句去捕捉异常，该语句在方法内部判断r.status_code是否等于200，如果不等于，则抛出异常。<br> 于是在这里我们有一个爬取网页的通用代码框架</p><pre><code>try:    r = requests.get(url, timeout=30)  # 请求超时时间为30秒    r.raise_for_status()  # 如果状态不是200，则引发异常    r.encoding = r.apparent_encoding  # 配置编码    print(r.text)except:    print(&quot;产生异常&quot;)</code></pre><h1 id="三、requests的综合小实例"><a href="#三、requests的综合小实例" class="headerlink" title="三、requests的综合小实例"></a>三、requests的综合小实例</h1><h3 id="实例一：京东商品信息的爬取"><a href="#实例一：京东商品信息的爬取" class="headerlink" title="实例一：京东商品信息的爬取"></a>实例一：京东商品信息的爬取</h3><pre><code>## 京东商品信息的爬取# 不需要对头部做任何修改，即可爬网页import requestsurl = &#39;http://item.jd.com/2967929.html&#39;try:    r = requests.get(url, timeout=30)    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[:1000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例二：亚马逊商品信息的爬取"><a href="#实例二：亚马逊商品信息的爬取" class="headerlink" title="实例二：亚马逊商品信息的爬取"></a>实例二：亚马逊商品信息的爬取</h3><pre><code>## 亚马逊商品信息的爬取# 该网页中对爬虫进行的爬取做了限制，因此我们需要伪装自己为浏览器发出的请求import requestsurl = &#39;http://www.amazon.cn/gp/product/B01M8L5Z3Y&#39;try:    kv = {&#39;user_agent&#39;: &#39;Mozilla/5.0&#39;}    r = requests.get(url, headers=kv)  # 改变自己的请求数据    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[1000:2000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例三：百度搜索关键字提交"><a href="#实例三：百度搜索关键字提交" class="headerlink" title="实例三：百度搜索关键字提交"></a>实例三：百度搜索关键字提交</h3><pre><code>## 百度搜索关键字提交# 百度的关键字接口：https://www.baidu.com/s?wd=keywordimport requestskeyword = &#39;python&#39;try:    kv = {&#39;wd&#39;: keyword}    headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}    r = requests.get(&#39;https://www.baidu.com/s&#39;, params=kv, headers=headers)    r.raise_for_status()    r.encoding = r.apparent_encoding    # print(len(r.text))    print(r.text)except:    print(&quot;失败&quot;)</code></pre><h3 id="实例四：网络图片的爬取"><a href="#实例四：网络图片的爬取" class="headerlink" title="实例四：网络图片的爬取"></a>实例四：网络图片的爬取</h3><pre><code>## 网络图片的爬取import requestsimport ostry:    url = &quot;https://odonohz90.qnssl.com/library/145456/bb0b3faa7a872d012bb4c57256b47585.jpg?imageView2/2/w/1000/h/1000/q/75&quot;  # 图片地址    root = r&quot;D:\DataguruPyhton\PythonSpider\lesson3\pic\\&quot;    path = root + url.split(&quot;/&quot;)[-1]    if not path.endswith(&quot;.jpg&quot;):        path += &quot;.jpg&quot;    if not os.path.exists(root):  # 目录不存在创建目录        os.mkdir(root)    if not os.path.exists(path):  # 文件不存在则下载        r = requests.get(url)        f = open(path, &quot;wb&quot;)        f.write(r.content)        f.close()        print(&quot;文件下载成功&quot;)    else:        print(&quot;文件已经存在&quot;)except:    print(&quot;获取失败&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> requests </tag>
            
            <tag> urllib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之二：环境部署、基础语法、文件操作</title>
      <link href="/2019/05/12/python-crawler-basic-oper-2/"/>
      <url>/2019/05/12/python-crawler-basic-oper-2/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Python的环境部署"><a href="#一、Python的环境部署" class="headerlink" title="一、Python的环境部署"></a>一、Python的环境部署</h1><p>Python安装、Python的IDE安装本文不再赘述，网上有很多教程</p><p>爬虫必备的几个库：Requests、Selenium、lxml、Beatiful Soup</p><ul><li>Requests 是基于urllib编写的第三方扩展库，是采用Apache2 Licensed开源协议的HTTP库</li><li>Selenium是一个自动化测试工具，利用它我们可以驱动浏览器执行特定的动作，如点击、下拉等操作。对于一些JavaScript渲染的页面来说，这种抓取方式非常有效。</li><li>lxml是Python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高</li><li>Beatiful Soup是Python的一个HTML或XML的解析库，我们可以用它来方便地从网页中提取数据</li></ul><h1 id="二、Python的基础语法"><a href="#二、Python的基础语法" class="headerlink" title="二、Python的基础语法"></a>二、Python的基础语法</h1><p>可参考我的《趣学Python——教孩子学编程》系列笔记</p><p><a href="https://www.jianshu.com/p/69b5c9ee926e" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第1-3章</a></p><p><a href="https://www.jianshu.com/p/00850c80f78f" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第4-6章</a></p><p><a href="https://www.jianshu.com/p/90d50cc592ed" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第7-8章</a></p><p><a href="https://www.jianshu.com/p/885cb1fa9827" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第9-10章</a></p><p><a href="https://www.jianshu.com/p/48715dcb524a" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第11-12章</a></p><p><a href="https://www.jianshu.com/p/c9850ca89b60" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第13章</a></p><h1 id="三、Python文件的读取与输出"><a href="#三、Python文件的读取与输出" class="headerlink" title="三、Python文件的读取与输出"></a>三、Python文件的读取与输出</h1><p>键盘输入</p><pre><code># 键盘输入（python3将raw_input和input进行了整合，只有input）str = input(&quot;Please enter:&quot;)print(&quot;你输入的内容是：&quot;, str)</code></pre><p>打开文件</p><pre><code># 打开一个文件fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;wb&quot;)print(&quot;文件名：&quot;, fo.name)print(&quot;是否已关闭：&quot;, fo.closed)print(&quot;访问模式：&quot;, fo.mode)</code></pre><p>关闭文件</p><pre><code># 关闭一个文件fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;wb&quot;)fo.close()print(&quot;是否已关闭：&quot;, fo.closed)</code></pre><p>写入文件内容</p><pre><code># 写入文件内容fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)fo.write(&quot;www.baidu.com www.cctvjiatao.com&quot;)fo.flush()fo.close()</code></pre><p>读取文件内容</p><pre><code>fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)print(&quot;读取的字符串是：&quot;, str)fo.close()</code></pre><p>查找当前位置</p><pre><code># 查找当前位置fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)position = fo.tell()print(&quot;当前读取的位置是：&quot;, position)# result: 当前文件位置： 11fo.close()</code></pre><p>文件指针重定位</p><pre><code># 文件指针重定位fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)print(&quot;读取的字符串1：&quot;, str)# result: 重新读取的字符串1： www.baidu.cposition = fo.tell()print(&quot;当前文件位置：&quot;, position)# result: 当前文件位置： 11str = fo.read(11)print(&quot;读取的字符串2：&quot;, str)# result: 读取的字符串2： om www.cctvpostion = fo.seek(0, 0)str = fo.read(11)print(&quot;读取的字符串3：&quot;, str)# result: 读取的字符串3： www.baidu.cfo.close()</code></pre><p>文件重命名</p><pre><code># 文件重命名 filejiatao.txt——&gt;filejiatao2.txtimport ossrc_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;dst_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao2.txt&quot;os.rename(src_file, dst_file)</code></pre><p>删除文件</p><pre><code># 删除一个文件import osdirty_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao2.txt&quot;os.remove(dirty_file)</code></pre><p> 异常处理1</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-71be88200641b051.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/627" alt="img"></p><p>2-1.jpg</p><pre><code># 异常处理1try:    fh = open(&quot;testfile.txt&quot;, &quot;w&quot;)    fh.write(&quot;this is my test file for exception handing!&quot;)except IOError:    print(&quot;Eorror:can\&#39;t find file or read data&quot;)else:    print(&quot;witten content in the file successfully&quot;)    fh.close()</code></pre><p>异常处理2</p><pre><code># 异常处理2try:    fh = open(&quot;testfile.txt&quot;, &quot;w&quot;)    fh.write(&quot;this is my test file for exception handing!&quot;)finally:    print(&quot;Eorror:I don\&#39;t kown why ...&quot;)</code></pre><p>异常处理3</p><pre><code># 异常处理3def temp_convert(var):    try:        return int(var)    # except ValueError,Argument:    #     print(&quot;The argument does not contain numbers\n&quot;,Argument)    except (ValueError) as Argument:        print(&quot;The argument does not contain numbers\n&quot;, Argument)temp_convert(&quot;xyz&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之一：网络爬虫理论基础</title>
      <link href="/2019/05/11/python-crawler-basic-internet-1/"/>
      <url>/2019/05/11/python-crawler-basic-internet-1/</url>
      
        <content type="html"><![CDATA[<h1 id="一、浏览网页的基本过程和通信基础"><a href="#一、浏览网页的基本过程和通信基础" class="headerlink" title="一、浏览网页的基本过程和通信基础"></a>一、浏览网页的基本过程和通信基础</h1><blockquote><p>当我们在浏览器地址栏输入： <a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a> 回车后会浏览器显示百度的首页，那这 段网络通信过程中到底发生了什么？</p></blockquote><p>简单来说这段过程发生了以下四个步骤：</p><ol><li>浏览器通过DNS服务器查找域名对应的IP地址;</li><li>向IP地址对应的Web服务器发送请求 ;</li><li>Web Web服务器响应请求，发回HTML页面 ;</li><li>浏览器解析HTML内容，并显示出来</li></ol><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><ul><li>DNS 是计算机域名系统 (Domain Name System (Domain Name System 或 Domain Name Service) 的缩写，由解析器和域名服务组成的。</li><li>域名服务器是指保存有该网络中所主机的和对应 IP 地址，并具有将域名转换为 IP 地址功能的服务器。</li><li>一般个域名的DNS解析时间在10~60毫秒之间。</li><li>一个域名必须对应IP地址，而一个IP地址不一定会有域名</li></ul><h3 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h3><ul><li>HTTP协议（HyperText Transfer Protocol，超文本传输协议）是一种发布和接收HTML页面的方法</li><li>HTTPS协议（HyperText Transfer Protocol over Secure Socket Layer）简单的讲就是HTTP的安全版，在HTTP下加入SSL层</li><li>SSL（Secure Socket Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全</li><li>HTTP的端口号是80，HTTPS的端口号是443</li></ul><h3 id="URI与URL"><a href="#URI与URL" class="headerlink" title="URI与URL"></a>URI与URL</h3><ul><li>URI（Uniform Resource Identifier）统一资源标志符</li><li>URL（Universal Resource Locator）统一资源定位符，用于完整地描述Internet上网页和其他资源的地址的一中标识方法</li><li>URL的基本格式：<a href="scheme://host[:port]/path/.../[?query-string][#anchor]" target="_blank" rel="noopener">scheme://host[:port]/path/.../[?query-string][#anchor]</a> </li></ul><h3 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h3><p>请求由客户端向服务端发出，分为四部分：请求方法、请求的网址、请求头、请求体</p><ul><li>请求方法</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-fe7eace52135769d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/751" alt="img"></p><p>  1-1.jpg</p><ul><li>请求头，用来说明服务器使用的附加信息，比较重要的信息有Cookie、Referer、user-Agent等。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-76c4af1982a66838.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/773" alt="img"></p><p>  1-2.jpg</p><ul><li>请求体，一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-569c648c2c6d1f89.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/819" alt="img"></p><p>  1-3.jpg</p><h3 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h3><p>响应由服务端返回给客户端，分为三部分：响应状态码、响应头、响应提</p><ul><li>响应状态码</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-5804a72cba62b6fd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>  1-4.jpg</p><ul><li>响应头，包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-a61e4fb72096b27e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>  1-4.jpg</p><ul><li>响应体，响应的正文数据都在响应体中，比如请求网页时他的响应体就是网页的HTML代码，请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的就是响应体</li></ul><h1 id="二、爬虫基本工作原理"><a href="#二、爬虫基本工作原理" class="headerlink" title="二、爬虫基本工作原理"></a>二、爬虫基本工作原理</h1><h3 id="爬虫基本类型"><a href="#爬虫基本类型" class="headerlink" title="爬虫基本类型"></a>爬虫基本类型</h3><ul><li>通用爬虫：是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</li><li>聚焦爬虫：是”面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于， 聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。</li><li>增量式爬虫：增量式更新指的是在更新的时候只更新改变的地方，而未改变的地方则不更新。所以增量式爬虫技术在爬取网页的过程中，只爬取内容发生变化或是新产生的网页，对未发生内容变化的网页则不会爬取。</li></ul><h3 id="爬虫的基本工作流程（以通用爬虫为例）"><a href="#爬虫的基本工作流程（以通用爬虫为例）" class="headerlink" title="爬虫的基本工作流程（以通用爬虫为例）"></a>爬虫的基本工作流程（以通用爬虫为例）</h3><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-cffe88e6b8b23391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/265" alt="img"></p><p>​      1-7.png</p><h5 id="第一步：抓取网页"><a href="#第一步：抓取网页" class="headerlink" title="第一步：抓取网页"></a>第一步：抓取网页</h5><ul><li>首先选取一部分的种子URL，将这些URL放入待抓取URL队列；</li><li>取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。</li><li>分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环</li></ul><h5 id="第二步：数据存储"><a href="#第二步：数据存储" class="headerlink" title="第二步：数据存储"></a>第二步：数据存储</h5><ul><li>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。</li><li>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。</li></ul><h5 id="第三步：预处理"><a href="#第三步：预处理" class="headerlink" title="第三步：预处理"></a>第三步：预处理</h5><ul><li>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理，比如：提取文字、中文分词、消除噪音（比如版权声明文字、导航条、广告等……）、索引处理、链接关系计算、特殊文件处理….</li><li>除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。</li><li>但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。</li></ul><h5 id="第四步：操作数据，实现需求"><a href="#第四步：操作数据，实现需求" class="headerlink" title="第四步：操作数据，实现需求"></a>第四步：操作数据，实现需求</h5><p>比如获取京东某类商品的所有评论、购买用户的会员等级</p><h3 id="爬虫基本结构"><a href="#爬虫基本结构" class="headerlink" title="爬虫基本结构"></a>爬虫基本结构</h3><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-f1dc7e8061ac95ed.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/813" alt="img"></p><p>1-6.jpg</p><h3 id="爬虫的抓取策略"><a href="#爬虫的抓取策略" class="headerlink" title="爬虫的抓取策略"></a>爬虫的抓取策略</h3><ul><li>深度优先遍历策略</li><li>广度优先遍历策略</li><li>方向链接策略</li><li>Partial PageRank 策略</li><li>OPIC策略</li></ul><h3 id="爬虫的更新策略"><a href="#爬虫的更新策略" class="headerlink" title="爬虫的更新策略"></a>爬虫的更新策略</h3><ul><li>历史参考策略</li><li>用户体验策略</li><li>聚类抽样策略</li></ul><h3 id="网页分析算法"><a href="#网页分析算法" class="headerlink" title="网页分析算法"></a>网页分析算法</h3><ul><li>基于用户行为的网页分析算法</li><li>基于网络拓扑的网页分析算法</li><li>基于网页内容的网页分析算法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
