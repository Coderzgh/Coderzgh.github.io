<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python网络爬虫实战之四：BeautifulSoup</title>
      <link href="/2019/05/15/python-crawler-regular-expression-5/"/>
      <url>/2019/05/15/python-crawler-regular-expression-5/</url>
      
        <content type="html"><![CDATA[<h2 id="正文："><a href="#正文：" class="headerlink" title="正文："></a>正文：</h2><p>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。</p><h2 id="通过一个小实例来了解正则表达式的作用"><a href="#通过一个小实例来了解正则表达式的作用" class="headerlink" title="通过一个小实例来了解正则表达式的作用"></a>通过一个小实例来了解正则表达式的作用</h2><pre><code># 从字符串 str 中找出abc/数字import res = &#39;123abc456eabc789&#39;re.findall(r&#39;abc&#39;, s)# result: [&#39;abc&#39;, &#39;abc&#39;]re.findall(&#39;[0-9]+&#39;,s)# result: [&#39;123&#39;, &#39;456&#39;, &#39;789&#39;]</code></pre><h2 id="限定符"><a href="#限定符" class="headerlink" title="限定符"></a>限定符</h2><p>限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-ae6d73e71049b6bb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>5-1.jpg</p><pre><code>import res = &#39;Chapter1 Chapter2 Chapter10 Chapter99 fheh&#39;re.findall(&#39;Chapter[1-9][0-9]*&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]+&#39;, s)# result: [&#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]?&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]{0,1}&#39;, s)# result:[&#39;Chapter1&#39;, &#39;Chapter2&#39;, &#39;Chapter10&#39;, &#39;Chapter99&#39;]re.findall(&#39;Chapter[1-9][0-9]{1,2}&#39;, s)# result:[&#39;Chapter10&#39;, &#39;Chapter99&#39;]</code></pre><h2 id="贪婪匹配与非贪婪匹配"><a href="#贪婪匹配与非贪婪匹配" class="headerlink" title="贪婪匹配与非贪婪匹配"></a>贪婪匹配与非贪婪匹配</h2><p>正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符</p><pre><code>import re# 贪婪模式s = &#39;&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;&#39;re.findall(&#39;&lt;.*&gt;&#39;, s)# result: [&#39;&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt;&#39;]# 非贪婪模式re.findall(&#39;&lt;.*?&gt;&#39;, s)# result: [&#39;&lt;H1&gt;&#39;, &#39;&lt;/H1&gt;&#39;]</code></pre><h2 id="定位符"><a href="#定位符" class="headerlink" title="定位符"></a>定位符</h2><p>定位符使您能够将正则表达式固定到行首或行尾。它们还使您能够创建这样的正则表达式，这些正则表达式出现在一个单词内、在一个单词的开头或者一个单词的结尾。</p><p>定位符用来描述字符串或单词的边界，^ 和 $ 分别指字符串的开始与结束，\b 描述单词的前或后边界，\B 表示非单词边界。</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-53ebbb5379727d24.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>5-2.jpg</p><pre><code>import res = &#39;Chapter1 Chapter2 Chapter11 Chapter99&#39;re.findall(&#39;^Chapter[1-9][0-9]{0,1}&#39;, s)# result: [&#39;Chapter1&#39;]re.findall(&#39;^Chapter[1-9][0-9]{0,1}$&#39;, &#39;Chapter99&#39;)# result: [&#39;Chapter99&#39;]re.findall(r&#39;\bCha&#39;, &#39; Chapter&#39;)# result: [&#39;Cha&#39;]re.findall(r&#39;ter\b&#39;, &#39; Chapter&#39;)# result: [&#39;ter&#39;]re.findall(r&#39;\Bapt&#39;, &#39;Chapter&#39;)# result: [&#39;apt&#39;]re.findall(r&#39;\Bapt&#39;, &#39;aptitude&#39;)# result: []</code></pre><h2 id="分组与捕获组"><a href="#分组与捕获组" class="headerlink" title="分组与捕获组"></a>分组与捕获组</h2><p>要说明白捕获，就要先从分组开始。重复单字符我们可以使用限定符，如果重复字符串，用什么呢？ 对！用小括号，小括号里包裹指定字表达式（子串），这就是分组。之后就可以限定这个子表示的重复次数了。</p><p>那么，什么是捕获呢？使用小括号指定一个子表达式后，匹配这个子表达式的文本（即匹配的内容）可以在表达式或者其他过程中接着用，怎么用呢？至少应该有个指针啥的引用它吧？ 对！默认情况下，每个分组（小括号）会自动拥有一个组号,从左到右，以分组的左括号为标志，第一个出现的分组组号为1，后续递增。如果出现嵌套，</p><pre><code>s = &#39;aaa111aaa , bbb222 , 333ccc&#39;re.findall(r&#39;[a-z]+(\d+)[a-z]&#39;, s)# result:[&#39;111&#39;]re.findall(r&#39;[a-z]+\d+[a-z]&#39;, s)  # 对比(\d+)的用法# result:[&#39;aaa111a&#39;]re.findall(r&#39;[a-z]+\d+[a-z]+&#39;, s)  # 对比(\d+)的用法# result:[&#39;aaa111aaa&#39;]s = &#39;111aaa222aaa111 , 333bbb444bb33&#39;re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, s)# result:[(&#39;111&#39;, &#39;aaa&#39;, &#39;222&#39;, &#39;aaa&#39;, &#39;111&#39;)]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, &#39;333bbb444bb33&#39;)# result:[]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\2)(\1)&#39;, &#39;333bbb444bbb333&#39;)# result:[(&#39;333&#39;, &#39;bbb&#39;, &#39;444&#39;, &#39;bbb&#39;, &#39;333&#39;)]re.findall(r&#39;(\d+)([a-z]+)(\d+)(\1)(\2)&#39;, &#39;333bbb444bbb333&#39;)# result:[]</code></pre><h2 id="非捕获组"><a href="#非捕获组" class="headerlink" title="非捕获组"></a>非捕获组</h2><p>用圆括号将所有选择项括起来，相邻的选择项之间用|分隔。但用圆括号会有一个副作用，使相关的匹配会被缓存。可用?:放在第一个选项前来消除这种副作用。</p><p>其中 ?: 是非捕获元之一，还有两个非捕获元是 ?= 和 ?!，这两个还有更多的含义，前者为正向预查，在任何开始匹配圆括号内的正则表达式模式的位置来匹配搜索字符串，后者为负向预查，在任何开始不匹配该正则表达式模式的位置来匹配搜索字符串。</p><pre><code># (?:pattern)与(pattern)不同之处只是在于不捕获结果，非捕获组只匹配结果，但不捕获结果，也不会分配组号s = &#39;industry is industries lala industyyy industiii&#39;re.findall(r&#39;industr(?:y|ies)&#39;, s)# result: [&#39;industry&#39;, &#39;industries&#39;]s = &#39;Windows2000 Windows3.1&#39;re.findall(r&#39;Windows(?=95|98|NT|2000)&#39;, s)# result: [&#39;Windows&#39;]# 匹配 &quot;Windows2000&quot; 中的 &quot;Windows&quot;,不匹配 &quot;Windows3.1&quot; 中的 &quot;Windows&quot;。s = &#39;Windows2000 Windows3.1&#39;re.findall(r&#39;Windows(?!95|98|NT|2000)&#39;, s)# result: [&#39;Windows&#39;]# 匹配 &quot;Windows3.1&quot; 中的 &quot;Windows&quot;,不匹配 &quot;Windows2000&quot; 中的 &quot;Windows&quot;。s = &#39;aaa111aaa,bbb222,333ccc,444ddd444,555eee666,fff777ggg&#39;re.findall(r&#39;([a-z]+)\d+([a-z]+)&#39;, s)# result:[(&#39;aaa&#39;, &#39;aaa&#39;), (&#39;fff&#39;, &#39;ggg&#39;)]re.findall(r&#39;(?P&lt;g1&gt;[a-z]+)\d+(?P=g1)&#39;, s)# result:[&#39;aaa&#39;]re.findall(r&#39;(?P&lt;g1&gt;[a-z]+)\d+(?P=g1)&#39;, &#39;aaa111aaa,bbb222,333ccc,444ddd444,555eee666,fff777fff&#39;)# result:[&#39;aaa&#39;, &#39;fff&#39;]re.findall(r&#39;[a-z]+(\d+)([a-z]+)&#39;, s)# result: [(&#39;111&#39;, &#39;aaa&#39;), (&#39;777&#39;, &#39;ggg&#39;)]re.findall(r&#39;([a-z]+)\d+&#39;, s)# result:[&#39;aaa&#39;, &#39;bbb&#39;, &#39;ddd&#39;, &#39;eee&#39;, &#39;fff&#39;]re.findall(r&#39;([a-z]+)\d+\1&#39;, s)# result:[&#39;aaa&#39;]s = &#39;I have a dog , I have a cat&#39;re.findall(r&#39;I have a (?:dog|cat)&#39;, s)# result: [&#39;I have a dog&#39;, &#39;I have a cat&#39;]re.findall(r&#39;I have a dog|cat&#39;, s)# result: [&#39;I have a dog&#39;, &#39;cat&#39;]s = &#39;ababab abbabb aabaab abbbbbab&#39;re.findall(r&#39;\b(?:ab)+\b&#39;, s)# result: [&#39;ababab&#39;]re.findall(r&#39;\b(ab)+\b&#39;, s)# result: [&#39;ab&#39;]</code></pre><h2 id="反向引用"><a href="#反向引用" class="headerlink" title="反向引用"></a>反向引用</h2><p>捕获组(Expression)在匹配成功时，会将子表达式匹配到的内容，保存到内存中一个以数字编号的组里，可以简单的认为是对一个局部变量进行了赋值，这时就可以通过反向引用方式，引用这个局部变量的值。一个捕获组(Expression)在匹配成功之前，它的内容可以是不确定的，一旦匹配成功，它的内容就确定了，反向引用的内容也就是确定的了。</p><p>反向引用必然要与捕获组一同使用的，如果没有捕获组，而使用了反向引用的语法，不同语言的处理方式不一致，有的语言会抛异常，有的语言会当作普通的转义处理。</p><pre><code>s = &#39;Is is the cost of of gasoline going up up ?&#39;re.findall(r&#39;\b([a-z]+) \1\b&#39;, s, re.I)  # 大小写不敏感# result: [&#39;Is&#39;, &#39;of&#39;, &#39;up&#39;]re.findall(r&#39;\b([a-z]+) \1\b&#39;, s)# result: [&#39;of&#39;, &#39;up&#39;]s = &#39;http://www.w3cschool.cc:80/html/html-tutorial.html&#39;re.findall(r&#39;(\w+):\/\/([^/:]+)(:\d*)?([^# ]*)&#39;, s)# result: [(&#39;http&#39;, &#39;www.w3cschool.cc&#39;, &#39;:80&#39;, &#39;/html/html-tutorial.html&#39;)]</code></pre><h2 id="注意区别：pattern-、pattern-、-pattern-、-pattern"><a href="#注意区别：pattern-、pattern-、-pattern-、-pattern" class="headerlink" title="注意区别：pattern+?、pattern*?、(?!pattern)、(?:pattern)"></a>注意区别：pattern+?、pattern*?、(?!pattern)、(?:pattern)</h2><h4 id="pattern-、pattern"><a href="#pattern-、pattern" class="headerlink" title="pattern+?、pattern*?"></a>pattern+?、pattern*?</h4><p>这两个比较常用，表示懒惰匹配，即匹配符合条件的尽量短的字符串。默认情况下 + 和 * 是贪婪匹配，即匹配尽可能长的字符串，在它们后面加上 ? 表示想要进行懒惰匹配。</p><h4 id="pattern"><a href="#pattern" class="headerlink" title="(?!pattern)"></a>(?!pattern)</h4><p>表示一个过滤条件，若字符串符合 pattern 则将其过滤掉。在分析日志时很有用，例如想过滤掉包含 info 标记的日志可以写 ^(?!.<em>info).</em>$。</p><h4 id="pattern-1"><a href="#pattern-1" class="headerlink" title="(?:pattern)"></a>(?:pattern)</h4><p>这条规则主要是为了优化性能，对匹配没有影响。它表示括号内的子表达式匹配的结果不需要返回也不会被 <img src="https://math.jianshu.com/math?formula=1" alt="1">2 之类的反向引用。</p><h2 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h2><ol><li>import re导入正则表达式模块。Python中所有正则表达式的函数都在re模块中，所以我们要先引入re模块；</li><li>用re.compile()函数创建一个Regex对象（参数就是要匹配的内容的正则表达式）；</li><li>用Regex对象的search()方法来查找一段字符串，返回那个匹配的对象num，num中是一段相应的描述信息；</li><li>调用匹配对象num的group()方法，返回实际匹配文本的字符串。</li></ol><pre><code>import res1 = &quot;once upon a time&quot;s2 = &quot;There once was a man from NewYork&quot;print(re.findall(r&#39;^once&#39;, s1))# result: [&#39;once&#39;]print(re.findall(r&#39;^once&#39;, s2))# result: []print(re.findall(r&#39;time$&#39;, s1))# result: [&#39;time&#39;]print(re.findall(r&#39;times$&#39;, s1))# result: []print(re.findall(r&#39;^time$&#39;, s1))# result: []print(re.findall(r&#39;^time$&#39;, &#39;time&#39;))# result: [&#39;time&#39;]## compiles = &#39;111,222,aaa,bbb,ccc333,444ddd&#39;rule = r&#39;\b\d+\b&#39;compiled_rule = re.compile(rule)print(compiled_rule.findall(s))# result: [&#39;111&#39;, &#39;222&#39;]## matchprint(re.match(&#39;www&#39;, &#39;www.runoob.com&#39;).span())  # 在起始位置匹配# result: (0, 3)print(re.match(&#39;com&#39;, &#39;www.runoob.com&#39;))  # 不在起始位置匹配# result: Noneline = &quot;Cats are smarter than dogs&quot;matchObj = re.match(r&#39;(.*) are (.*?) .*&#39;, line, re.M | re.I)if matchObj:    print(&quot;matchObj.group() : &quot;, matchObj.group())    # result: matchObj.group() :  Cats are smarter than dogs    print(&quot;matchObj.group(1) : &quot;, matchObj.group(1))    # result: matchObj.group(1) :  Cats    print(&quot;matchObj.group(2) : &quot;, matchObj.group(2))    # result: matchObj.group(2) :  smarterelse:    print(&quot;No match!!&quot;)## searchprint(re.search(&#39;www&#39;, &#39;www.runoob.com&#39;).span())  # 在起始位置匹配# result: (0, 3)print(re.search(&#39;com&#39;, &#39;www.runoob.com&#39;).span())  # 不在起始位置匹配# result: (11, 14)line = &quot;Cats are smarter than dogs&quot;;searchObj = re.search(r&#39;(.*) are (.*?) .*&#39;, line, re.M | re.I)if searchObj:    print(&quot;searchObj.group() : &quot;, searchObj.group())    # result: searchObj.group() :  Cats are smarter than dogs    print(&quot;searchObj.group(1) : &quot;, searchObj.group(1))    # result: searchObj.group(1) :  Cats    print(&quot;searchObj.group(2) : &quot;, searchObj.group(2))    # result: searchObj.group(2) :  smarterelse:    print(&quot;Nothing found!!&quot;)## match与searchline = &quot;Cats are smarter than dogs&quot;matchObj = re.match(r&#39;dogs&#39;, line, re.M | re.I)if matchObj:    print(&quot;match --&gt; matchObj.group() : &quot;, matchObj.group())else:    print(&quot;No match!!&quot;)    # result: No match!!matchObj = re.search(r&#39;dogs&#39;, line, re.M | re.I)if matchObj:    print(&quot;search --&gt; matchObj.group() : &quot;, matchObj.group())    # result: search --&gt; matchObj.group() :  dogselse:    print(&quot;No match!!&quot;)## subphone = &quot;2004-959-559 # This is Phone Number&quot;num = re.sub(r&#39;#.*$&#39;, &quot;&quot;, phone)print(&quot;Phone Num : &quot;, num)# result: Phone Num :  2004-959-559num = re.sub(r&#39;\D&#39;, &quot;&quot;, phone)print(&quot;Phone Num : &quot;, num)# result: Phone Num :  2004959559</code></pre><h2 id="正则表达式与BeautifulSoup"><a href="#正则表达式与BeautifulSoup" class="headerlink" title="正则表达式与BeautifulSoup"></a>正则表达式与BeautifulSoup</h2><pre><code>from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObj = BeautifulSoup(html, &#39;lxml&#39;)# print(bsObj.prettify())images = bsObj.findAll(&quot;img&quot;, {&quot;src&quot;: re.compile(&quot;\.\.\/img\/gifts/img.*\.jpg&quot;)})for image in images:    print(image[&quot;src&quot;])# result: ../img/gifts/img1.jpg# ../img/gifts/img2.jpg# ../img/gifts/img3.jpg# ../img/gifts/img4.jpg# ../img/gifts/img6.jpg</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> regex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之四：BeautifulSoup</title>
      <link href="/2019/05/14/python-crawler-beautifulsoup-4/"/>
      <url>/2019/05/14/python-crawler-beautifulsoup-4/</url>
      
        <content type="html"><![CDATA[<h2 id="正文："><a href="#正文：" class="headerlink" title="正文："></a>正文：</h2><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.</p><p>安装: pip install beautifulsoup4</p><blockquote><p>名字是beautifulsoup 的包,是 Beautiful Soup3 的发布版本,因为很多项目还在使用BS3, 所以 beautifulsoup 包依然有效.但是如果你在编写新项目,那么你应该安装的 beautifulsoup4，这个包兼容Python2和Python3</p></blockquote><h2 id="BeautifulSoup的基础使用"><a href="#BeautifulSoup的基础使用" class="headerlink" title="BeautifulSoup的基础使用"></a>BeautifulSoup的基础使用</h2><p>下面的一段HTML代码将作为例子被多次用到.这是《爱丽丝梦游仙境》的的一段内容(以后简称文档)</p><pre><code>html_doc = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;</code></pre><p>使用BeautifulSoup解析这段代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出</p><pre><code>from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, &quot;lxml&quot;)print(soup.prettify())# result:# &lt;html&gt;#  &lt;head&gt;#   &lt;title&gt;#    The Dormouse&#39;s story#   &lt;/title&gt;#  &lt;/head&gt;#  &lt;body&gt;#   &lt;p class=&quot;title&quot;&gt;#    &lt;b&gt;#     The Dormouse&#39;s story#    &lt;/b&gt;#   &lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;#    Once upon a time there were three little sisters; and their names were#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;#     Elsie#    &lt;/a&gt;#    ,#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;#     Lacie#    &lt;/a&gt;#    and#    &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;#     Tillie#    &lt;/a&gt;#    ;# and they lived at the bottom of a well.#   &lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;#    ...#   &lt;/p&gt;#  &lt;/body&gt;# &lt;/html&gt;</code></pre><p>几个简单的浏览结构化数据的方法</p><pre><code>soup.title# result: &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;soup.title.name# result: &#39;title&#39;soup.title.string# result: &#39;The Dormouse&#39;s story&#39;soup.title.text# result: &#39;The Dormouse&#39;s story&#39;soup.title.parent.name# result: &#39;head&#39;soup.p# result: &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;soup.p[&#39;class&#39;]# result: &#39;title&#39;soup.a# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;soup.find_all(&#39;a&#39;)#  result:# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,#  &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]soup.find(id=&quot;link3&quot;)# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre><p>从文档中找到所有<a>标签的链接</a></p><pre><code>for link in soup.find_all(&#39;a&#39;):    print(link.get(&#39;href&#39;))# result:# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie</code></pre><p>从文档中获取所有文字内容</p><pre><code>print(soup.get_text())# result:# The Dormouse&#39;s story## The Dormouse&#39;s story## Once upon a time there were three little sisters; and their names were# Elsie,# Lacie and# Tillie;# and they lived at the bottom of a well.## ...</code></pre><p>从文档中解析导航树</p><pre><code># 直接子节点print(soup.body.contents)for child in soup.descendants:    print(child)# 所有后代节点for child in soup.descendants:    print(child)# 节点内容1-包含换行符for string in soup.strings:    print(repr(string))# 节点内容2-不包含换行符for string in soup.stripped_strings:    print(repr(string))# 兄弟节点(没有一个兄弟节点会返回None)print(soup.p.next_sibling)print(soup.p.previous_sibling)for sibling in soup.a.next_siblings:    print(repr(sibling))# 前后节点print(soup.head.next_element)print(soup.head.previous_element)# 父节点from urllib.request import urlopenhtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObj = BeautifulSoup(html)print(bsObj.find(&quot;img&quot;, {&quot;src&quot;: &quot;../img/gifts/img1.jpg&quot;}).parent.previous_sibling.get_text())</code></pre><p>从文档中解析CSS选择器</p><pre><code>print(soup.select(&#39;title&#39;))print(soup.select(&#39;a&#39;))print(soup.select(&#39;b&#39;))print(soup.select(&#39;.sisiter&#39;))print(soup.select(&#39;#link1&#39;))print(soup.select(&#39;p #link1&#39;))print(soup.select(&#39;head &gt; title&#39;))print(soup.select(&#39;a[class=&quot;sister&quot;]&#39;))print(soup.select(&#39;a[href=&quot;http://example.com/elsie&quot;]&#39;))print(soup.select(&#39;p a[href=&quot;http://example.com/elsie&quot;]&#39;))</code></pre><p>结合正则表达式或其他逻辑条件</p><pre><code>import refor tag in soup.find_all(href=re.compile(&quot;elsie&quot;)):    print(tag)# result: &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;for tag in soup.find_all(&quot;a&quot;, class_=&quot;sister&quot;):    print(tag)# result：# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;for tag in soup.find_all([&quot;a&quot;, &quot;b&quot;]):    print(tag)# result :# &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;for tag in soup.find_all(True):    print(tag)# result:# &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;# &lt;body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;# &lt;/body&gt;&lt;/html&gt;# &lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;# &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;# &lt;body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;# &lt;/body&gt;# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;def has_class_but_no_id(tag):    return tag.has_attr(&#39;class&#39;) and not tag.has_attr(&#39;id&#39;)for tag in soup.find_all(has_class_but_no_id):    print(tag)# result:# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;# and they lived at the bottom of a well.&lt;/p&gt;# &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</code></pre><h2 id="BeautifulSoup的主要参数的使用"><a href="#BeautifulSoup的主要参数的使用" class="headerlink" title="BeautifulSoup的主要参数的使用"></a>BeautifulSoup的主要参数的使用</h2><pre><code>html_doc = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot;import bs4from bs4 import BeautifulSoupsoup = BeautifulSoup(html_doc, &quot;lxml&quot;)# text参数soup.find_all(text=&quot;Elsie&quot;)soup.find_all(text=[&quot;Elsie&quot;, &quot;Lacie&quot;, &quot;Tillie&quot;])soup.find_all(text=re.compile(&quot;Dormouse&quot;))# limit参数soup.find_all(&quot;a&quot;, limit=2)# recursive参数soup.html.find_all(&quot;title&quot;)soup.html.find_all(&quot;title&quot;, recursive=False)# tag对象print(soup.title)print(soup.head)print(soup.a)print(soup.p)print(type(soup.a))print(soup.name)print(soup.head.name)print(soup.p.attrs)print(soup.p[&#39;class&#39;])print(soup.p.get(&#39;class&#39;))soup.p[&#39;class&#39;] = &quot;newclass&quot;print(soup.p)del soup.p[&#39;class&#39;]print(soup.p)# NavigableString对象print(soup.p.string)print(type(soup.p.string))# BeautifulSoup对象print(type(soup.name))print(soup.name)print(soup.attrs)# Comment对象print(soup.a)print(soup.a.string)print(type(soup.a.string))if type(soup.a.string) == bs4.element.Comment:    print(soup.a.string)</code></pre><h2 id="BeautifulSoup解析在线网页实例"><a href="#BeautifulSoup解析在线网页实例" class="headerlink" title="BeautifulSoup解析在线网页实例"></a>BeautifulSoup解析在线网页实例</h2><pre><code>from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/exercises/exercise1.html&#39;)bsObj = BeautifulSoup(html.read(), &#39;lxml&#39;)print(bsObj.h1)# CSS属性from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/pages/warandpeace.html&#39;)bsObj = BeautifulSoup(html, &#39;lxml&#39;)nameList = bsObj.findAll(&#39;span&#39;, {&#39;class&#39;: &#39;green&#39;})for name in nameList:    print(name.get_text())# find()和findall()from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&#39;http://www.pythonscraping.com/pages/warandpeace.html&#39;)bsObj = BeautifulSoup(html)allText = bsObj.findAll(id=&#39;text&#39;)print(allText[0].get_text())</code></pre><p>更多使用方法参见<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">BeautifulSoup 中文文档</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之三：基本工具库urllib和requests</title>
      <link href="/2019/05/13/python-crawler-urllib-requests-3/"/>
      <url>/2019/05/13/python-crawler-urllib-requests-3/</url>
      
        <content type="html"><![CDATA[<h1 id="Python网络爬虫实战之三：基本工具库urllib和requests"><a href="#Python网络爬虫实战之三：基本工具库urllib和requests" class="headerlink" title="Python网络爬虫实战之三：基本工具库urllib和requests"></a>Python网络爬虫实战之三：基本工具库urllib和requests</h1><h1 id="一、urllib"><a href="#一、urllib" class="headerlink" title="一、urllib"></a>一、urllib</h1><h3 id="urllib简介"><a href="#urllib简介" class="headerlink" title="urllib简介"></a>urllib简介</h3><p>urllib是Python中一个功能强大用于操作URL，并在爬虫时经常用到的一个基础库，无需额外安装，默认已经安装到python中。</p><h3 id="urllib在python2-x与python3-x中的区别"><a href="#urllib在python2-x与python3-x中的区别" class="headerlink" title="urllib在python2.x与python3.x中的区别"></a>urllib在python2.x与python3.x中的区别</h3><p>在python2.x中，urllib分为urllib和urllib2，在python3.x中合并为urllib。两者使用起来不太一样，注意转换。</p><table><thead><tr><th>Python2.x</th><th>Python3.x</th></tr></thead><tbody><tr><td>import urllib2</td><td>import urllib.request，urllib.error</td></tr><tr><td>import urllib</td><td>import urllib.request，urllib.error，urllib.parse</td></tr><tr><td>import urlparse</td><td>import urllib.parse</td></tr><tr><td>import urlopen</td><td>import urllib.request.urlopen</td></tr><tr><td>import urlencode</td><td>import urllib.parse.urlencode</td></tr><tr><td>import urllib.quote</td><td>import urllib.request.quote</td></tr><tr><td>cookielib.CookieJar</td><td>http.CookieJar</td></tr><tr><td>urllib2.Request</td><td>urllib.request.Request</td></tr></tbody></table><h3 id="urllib的四个子模块"><a href="#urllib的四个子模块" class="headerlink" title="urllib的四个子模块"></a>urllib的四个子模块</h3><p>Python3.6.0中urllib模块包括一下四个子模块，urllib模块是一个运用于URL的包（urllib is a package that collects several modules for working with URLs）</p><ul><li>urllib.request用于访问和读取URLS（urllib.request for opening and reading URLs），就像在浏览器里输入网址然后回车一样，只需要给这个库方法传入URL和其他参数就可以模拟实现这个过程。</li><li>urllib.error包括了所有urllib.request导致的异常（urllib.error containing the exceptions raised by urllib.request），我们可以捕捉这些异常，然后进行重试或者其他操作以确保程序不会意外终止。</li><li>urllib.parse用于解析URLS（urllib.parse for parsing URLs），提供了很多URL处理方法，比如拆分、解析、合并、编码。</li><li>urllib.robotparser用于解析robots.txt文件（urllib.robotparser for parsing robots.txt files），然后判断哪些网站可以爬，哪些网站不可以爬。</li></ul><h3 id="使用urllib打开网页"><a href="#使用urllib打开网页" class="headerlink" title="使用urllib打开网页"></a>使用urllib打开网页</h3><p>最基本的方法打开网页</p><pre><code># 最基本的方法打开网页from urllib.request import urlopenresponse = urlopen(&quot;http://www.baidu.com&quot;)print(type(response))print(response.status)print(response.getheaders())print(response.getheader(&#39;Server&#39;))html = response.read()print(html)</code></pre><p>携带data参数打开网页</p><pre><code># 携带data参数打开网页from urllib.parse import urlencodefrom urllib.request import urlopendata = bytes(urlencode({&#39;word&#39;: &#39;hello&#39;}), encoding=&#39;utf8&#39;)response = urlopen(&#39;http://httpbin.org/post&#39;, data=data)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>携带timeout参数打开网页1</p><pre><code>#  携带timeout参数打开网页1from urllib.request import urlopen# response = urllib.request.urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=1)print(response.read())</code></pre><p>携带timeout参数打开网页2</p><pre><code># 携带timeout参数打开网页2from urllib.request import urlopentry:    response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)    print(response.read())except Exception as e:    print(e)</code></pre><p>通过构建Request打开网页1</p><pre><code># 通过构建Request打开网页1from urllib.request import Requestfrom urllib.request import urlopenrequest = Request(&#39;https://python.org&#39;)response = urlopen(request)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页2</p><pre><code># 通过构建Request打开网页2from urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;headers = {    &#39;User-Agent&#39;: &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;,    &#39;Host&#39;: &#39;httpbin.org&#39;}dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, headers=headers, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>与通过构建Request打开网页2对比</p><pre><code># 与通过构建Request打开网页2对比from urllib.request import Requestfrom urllib.request import urlopenreq = Request(url=url, data=data, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页3：通过add_header方</p><pre><code># 通过构建Request打开网页3：通过add_header方法添加headersfrom urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, method=&#39;POST&#39;)req.add_header(&#39;User-Agent&#39;, &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>urlencode()的使用</p><pre><code># urlencode()的使用from urllib.parse import urlencodefrom urllib.request import urlopendata = {&#39;first&#39;: &#39;true&#39;, &#39;pn&#39;: 1, &#39;kd&#39;: &#39;Python&#39;}data = urlencode(data).encode(&#39;utf-8&#39;)datapage = urlopen(req, data=data).read()page</code></pre><p>使用代理打开网页</p><pre><code># 使用代理from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy_handler = ProxyHandler({&#39;http&#39;: &#39;106.56.102.140:8070&#39;})opener = build_opener(proxy_handler)try:    response = opener.open(&#39;http://www.baidu.com/&#39;)    print(response.read().decode(&#39;utf-8&#39;))except URLError as e:    print(e.reason)</code></pre><h1 id="二、requests"><a href="#二、requests" class="headerlink" title="二、requests"></a>二、requests</h1><p>相比较urllib模块，requests模块要简单很多，但是需要单独安装：</p><ul><li>在windows系统下只需要在命令行输入命令 pip install requests 即可安装。</li><li>在 linux 系统下，只需要输入命令 sudo pip install requests ，即可安装。</li></ul><p>requests库的八个主要方法</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>requests.request()</td><td>构造一个请求，支持以下各种方法</td></tr><tr><td>requests.get()</td><td>向html网页提交get请求的方法</td></tr><tr><td>requests.post()</td><td>向html网页提交post请求的方法</td></tr><tr><td>requests.head()</td><td>获取html头部信息的主要方法</td></tr><tr><td>requests.put()</td><td>向html网页提交put请求的方法</td></tr><tr><td>requests.options()</td><td>向html网页提交options请求的方法</td></tr><tr><td>requests.patch()</td><td>向html网页提交局部修改的请求</td></tr><tr><td>requests.delete()</td><td>向html网页提交删除的请求</td></tr></tbody></table><p>请求之后，服务器通过response返回数据，response具体参数如下图：</p><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>r.status_code</td><td>http请求的返回状态，若为200则表示请求成功</td></tr><tr><td>r.text</td><td>http响应内容的字符串形式，即返回的页面内容</td></tr><tr><td>r.encoding</td><td>从http header 中猜测的相应内容编码方式</td></tr><tr><td>r.apparent_encoding</td><td>从内容中分析出的响应内容编码方式（备选编码方式）</td></tr><tr><td>r.content</td><td>http响应内容的二进制形式</td></tr></tbody></table><h3 id="requests-request-method-url-kwargs"><a href="#requests-request-method-url-kwargs" class="headerlink" title="requests.request(method, url, **kwargs)"></a>requests.request(method, url, **kwargs)</h3><ul><li>method：即 get、post、head、put、options、patch、delete</li><li>url：即请求的网址</li><li>**kwargs：控制访问的参数，具体参数如下：</li><li><ul><li>params：字典或字节序列，作为参数增加到url中。使用这个参数可以把一些键值对以?key1=value1&amp;key2=value2的模式增加到url中</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 paramsimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>data：字典，字节序或文件对象，重点作为向服务器提供或提交资源是提交，作为request的内容，与params不同的是，data提交的数据并不放在url链接里， 而是放在url链接对应位置的地方作为数据来存储。它也可以接受一个字符串对象。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 dataimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, data=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;23&quot;, #     &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>json：json格式的数据， json合适在相关的html，http相关的web开发中非常常见， 也是http最经常使用的数据格式， 他是作为内容部分可以向服务器提交。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 jsonimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, json=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;{\&quot;key1\&quot;: \&quot;value1\&quot;, \&quot;key2\&quot;: \&quot;value2\&quot;}&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;36&quot;, #     &quot;Content-Type&quot;: &quot;application/json&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>files：字典， 是用来向服务器传输文件时使用的字段。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 fileimport requests# filejiatao.txt 文件的内容是文本“www.baidu.com www.cctvjiatao.com”files = {&#39;file&#39;: open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;rb&quot;)}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, files=files)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {#     &quot;file&quot;: &quot;www.baidu.com www.cctvjiatao.com&quot;#   }, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;182&quot;, #     &quot;Content-Type&quot;: &quot;multipart/form-data; boundary=ee12ea6a4fd2b8a3318566775f2b268f&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>headers：字典是http的相关语，对应了向某个url访问时所发起的http的头字段， 可以用这个字段来定义http的访问的http头，可以用来模拟任何我们想模拟的浏览器来对url发起访问。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 headersimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload, headers=headers)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>cookies：字典或CookieJar，指的是从http中解析cookie</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 cookiesimport requestscookies = dict(cookies_are=&#39;working&#39;)r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/cookies&#39;, cookies=cookies)print(r.url)# result: http://httpbin.org/cookiesprint(r.text)# result:# {#   &quot;cookies&quot;: {#     &quot;cookies_are&quot;: &quot;working&quot;#   }# }</code></pre><ul><li><ul><li>auth：元组，用来支持http认证功能</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 authimport requestscs_user = &#39;用户名&#39;cs_psw = &#39;密码&#39;r = requests.request(&#39;GET&#39;, &#39;https://api.github.com&#39;, auth=(cs_user, cs_psw))print(r.url)# result: 待补充print(r.text)# result: 待补充</code></pre><ul><li><ul><li>timeout: 用于设定超时时间， 单位为秒，当发起一个get请求时可以设置一个timeout时间， 如果在timeout时间内请求内容没有返回， 将产生一个timeout的异常。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 timeoutimport requestsr = requests.request(&#39;GET&#39;, &#39;http://github.com&#39;, timeout=0.001)print(r.url)# result: 报错 socket.timeout: timed out</code></pre><ul><li><ul><li>proxies：字典， 用来设置访问代理服务器。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 proxiesimport requestsproxies = {    &#39;https&#39;: &#39;http://41.118.132.69:4433&#39;}# 也可以通过环境变量设置代理# export HTTP_PROXY=&#39;http://10.10.1.10:3128&#39;# export HTTPS_PROXY=&#39;http://10.10.1.10:1080&#39;r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, proxies=proxies)print(r.url)# result: http://httpbin.org/getprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get&quot;# }</code></pre><ul><li><ul><li>verify：开关， 用于认证SSL证书， 默认为True</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 verify，SSL证书验证import requestsr = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=True)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=False)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://github.com&#39;, verify=True)print(r.text)</code></pre><ul><li><ul><li>allow_redirects: 开关， 表示是否允许对url进行重定向， 默认为True。</li></ul></li><li><ul><li>stream: 开关， 指是否对获取内容进行立即下载， 默认为True。</li></ul></li><li><ul><li>cert： 用于设置保存本地SSL证书路径</li></ul></li></ul><h3 id="requests-get-url-params-None-kwargs"><a href="#requests-get-url-params-None-kwargs" class="headerlink" title="requests.get(url, params=None, **kwargs)"></a>requests.get(url, params=None, **kwargs)</h3><pre><code># 官方文档def get(url, params=None, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;get&#39;, url, params=params, **kwargs)</code></pre><h3 id="requests-post-url-data-None-json-None-kwargs"><a href="#requests-post-url-data-None-json-None-kwargs" class="headerlink" title="requests.post(url, data=None, json=None, **kwargs)"></a>requests.post(url, data=None, json=None, **kwargs)</h3><pre><code># 官方文档def post(url, data=None, json=None, **kwargs):    return request(&#39;post&#39;, url, data=data, json=json, **kwargs)</code></pre><h3 id="requests-head-url-kwargs"><a href="#requests-head-url-kwargs" class="headerlink" title="requests.head(url, **kwargs)"></a>requests.head(url, **kwargs)</h3><pre><code># 官方文档def head(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, False)    return request(&#39;head&#39;, url, **kwargs)</code></pre><h3 id="requests-options-url-kwargs"><a href="#requests-options-url-kwargs" class="headerlink" title="requests.options(url, **kwargs)"></a>requests.options(url, **kwargs)</h3><pre><code># 官方文档def options(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;options&#39;, url, **kwargs)</code></pre><h3 id="requests-put-url-data-None-kwargs"><a href="#requests-put-url-data-None-kwargs" class="headerlink" title="requests.put(url, data=None, **kwargs)"></a>requests.put(url, data=None, **kwargs)</h3><pre><code># 官方文档def put(url, data=None, **kwargs):    return request(&#39;put&#39;, url, data=data, **kwargs)</code></pre><h3 id="requests-patch-url-data-None-kwargs"><a href="#requests-patch-url-data-None-kwargs" class="headerlink" title="requests.patch(url, data=None, **kwargs)"></a>requests.patch(url, data=None, **kwargs)</h3><pre><code># 官方文档def patch(url, data=None, **kwargs):    return request(&#39;patch&#39;, url, data=data, **kwargs)</code></pre><blockquote><p>requests.patch和request.put类似。<br> 两者不同的是： 当我们用patch时仅需要提交需要修改的字段。<br> 而用put时，必须将所有字段一起提交到url，未提交字段将会被删除。<br> patch的好处是：节省网络带宽。</p></blockquote><h3 id="requests-delete-url-kwargs"><a href="#requests-delete-url-kwargs" class="headerlink" title="requests.delete(url, **kwargs)"></a>requests.delete(url, **kwargs)</h3><pre><code># 官方文档def delete(url, **kwargs):    return request(&#39;delete&#39;, url, **kwargs)</code></pre><h3 id="requests库的异常"><a href="#requests库的异常" class="headerlink" title="requests库的异常"></a>requests库的异常</h3><p>注意requests库有时会产生异常，比如网络连接错误、http错误异常、重定向异常、请求url超时异常等等。所以我们需要判断r.status_codes是否是200，在这里我们怎么样去捕捉异常呢？<br> 这里我们可以利用r.raise_for_status() 语句去捕捉异常，该语句在方法内部判断r.status_code是否等于200，如果不等于，则抛出异常。<br> 于是在这里我们有一个爬取网页的通用代码框架</p><pre><code>try:    r = requests.get(url, timeout=30)  # 请求超时时间为30秒    r.raise_for_status()  # 如果状态不是200，则引发异常    r.encoding = r.apparent_encoding  # 配置编码    print(r.text)except:    print(&quot;产生异常&quot;)</code></pre><h1 id="三、requests的综合小实例"><a href="#三、requests的综合小实例" class="headerlink" title="三、requests的综合小实例"></a>三、requests的综合小实例</h1><h3 id="实例一：京东商品信息的爬取"><a href="#实例一：京东商品信息的爬取" class="headerlink" title="实例一：京东商品信息的爬取"></a>实例一：京东商品信息的爬取</h3><pre><code>## 京东商品信息的爬取# 不需要对头部做任何修改，即可爬网页import requestsurl = &#39;http://item.jd.com/2967929.html&#39;try:    r = requests.get(url, timeout=30)    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[:1000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例二：亚马逊商品信息的爬取"><a href="#实例二：亚马逊商品信息的爬取" class="headerlink" title="实例二：亚马逊商品信息的爬取"></a>实例二：亚马逊商品信息的爬取</h3><pre><code>## 亚马逊商品信息的爬取# 该网页中对爬虫进行的爬取做了限制，因此我们需要伪装自己为浏览器发出的请求import requestsurl = &#39;http://www.amazon.cn/gp/product/B01M8L5Z3Y&#39;try:    kv = {&#39;user_agent&#39;: &#39;Mozilla/5.0&#39;}    r = requests.get(url, headers=kv)  # 改变自己的请求数据    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[1000:2000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例三：百度搜索关键字提交"><a href="#实例三：百度搜索关键字提交" class="headerlink" title="实例三：百度搜索关键字提交"></a>实例三：百度搜索关键字提交</h3><pre><code>## 百度搜索关键字提交# 百度的关键字接口：https://www.baidu.com/s?wd=keywordimport requestskeyword = &#39;python&#39;try:    kv = {&#39;wd&#39;: keyword}    headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}    r = requests.get(&#39;https://www.baidu.com/s&#39;, params=kv, headers=headers)    r.raise_for_status()    r.encoding = r.apparent_encoding    # print(len(r.text))    print(r.text)except:    print(&quot;失败&quot;)</code></pre><h3 id="实例四：网络图片的爬取"><a href="#实例四：网络图片的爬取" class="headerlink" title="实例四：网络图片的爬取"></a>实例四：网络图片的爬取</h3><pre><code>## 网络图片的爬取import requestsimport ostry:    url = &quot;https://odonohz90.qnssl.com/library/145456/bb0b3faa7a872d012bb4c57256b47585.jpg?imageView2/2/w/1000/h/1000/q/75&quot;  # 图片地址    root = r&quot;D:\DataguruPyhton\PythonSpider\lesson3\pic\\&quot;    path = root + url.split(&quot;/&quot;)[-1]    if not path.endswith(&quot;.jpg&quot;):        path += &quot;.jpg&quot;    if not os.path.exists(root):  # 目录不存在创建目录        os.mkdir(root)    if not os.path.exists(path):  # 文件不存在则下载        r = requests.get(url)        f = open(path, &quot;wb&quot;)        f.write(r.content)        f.close()        print(&quot;文件下载成功&quot;)    else:        print(&quot;文件已经存在&quot;)except:    print(&quot;获取失败&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> requests </tag>
            
            <tag> urllib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之二：环境部署、基础语法、文件操作</title>
      <link href="/2019/05/12/python-crawler-basic-oper-2/"/>
      <url>/2019/05/12/python-crawler-basic-oper-2/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Python的环境部署"><a href="#一、Python的环境部署" class="headerlink" title="一、Python的环境部署"></a>一、Python的环境部署</h1><p>Python安装、Python的IDE安装本文不再赘述，网上有很多教程</p><p>爬虫必备的几个库：Requests、Selenium、lxml、Beatiful Soup</p><ul><li>Requests 是基于urllib编写的第三方扩展库，是采用Apache2 Licensed开源协议的HTTP库</li><li>Selenium是一个自动化测试工具，利用它我们可以驱动浏览器执行特定的动作，如点击、下拉等操作。对于一些JavaScript渲染的页面来说，这种抓取方式非常有效。</li><li>lxml是Python的一个解析库，支持HTML和XML的解析，支持XPath解析方式，而且解析效率非常高</li><li>Beatiful Soup是Python的一个HTML或XML的解析库，我们可以用它来方便地从网页中提取数据</li></ul><h1 id="二、Python的基础语法"><a href="#二、Python的基础语法" class="headerlink" title="二、Python的基础语法"></a>二、Python的基础语法</h1><p>可参考我的《趣学Python——教孩子学编程》系列笔记</p><p><a href="https://www.jianshu.com/p/69b5c9ee926e" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第1-3章</a></p><p><a href="https://www.jianshu.com/p/00850c80f78f" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第4-6章</a></p><p><a href="https://www.jianshu.com/p/90d50cc592ed" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第7-8章</a></p><p><a href="https://www.jianshu.com/p/885cb1fa9827" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第9-10章</a></p><p><a href="https://www.jianshu.com/p/48715dcb524a" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第11-12章</a></p><p><a href="https://www.jianshu.com/p/c9850ca89b60" target="_blank" rel="noopener">《趣学Python——教孩子学编程》学习笔记第13章</a></p><h1 id="三、Python文件的读取与输出"><a href="#三、Python文件的读取与输出" class="headerlink" title="三、Python文件的读取与输出"></a>三、Python文件的读取与输出</h1><p>键盘输入</p><pre><code># 键盘输入（python3将raw_input和input进行了整合，只有input）str = input(&quot;Please enter:&quot;)print(&quot;你输入的内容是：&quot;, str)</code></pre><p>打开文件</p><pre><code># 打开一个文件fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;wb&quot;)print(&quot;文件名：&quot;, fo.name)print(&quot;是否已关闭：&quot;, fo.closed)print(&quot;访问模式：&quot;, fo.mode)</code></pre><p>关闭文件</p><pre><code># 关闭一个文件fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;wb&quot;)fo.close()print(&quot;是否已关闭：&quot;, fo.closed)</code></pre><p>写入文件内容</p><pre><code># 写入文件内容fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)fo.write(&quot;www.baidu.com www.cctvjiatao.com&quot;)fo.flush()fo.close()</code></pre><p>读取文件内容</p><pre><code>fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)print(&quot;读取的字符串是：&quot;, str)fo.close()</code></pre><p>查找当前位置</p><pre><code># 查找当前位置fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)position = fo.tell()print(&quot;当前读取的位置是：&quot;, position)# result: 当前文件位置： 11fo.close()</code></pre><p>文件指针重定位</p><pre><code># 文件指针重定位fo = open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;r+&quot;)str = fo.read(11)print(&quot;读取的字符串1：&quot;, str)# result: 重新读取的字符串1： www.baidu.cposition = fo.tell()print(&quot;当前文件位置：&quot;, position)# result: 当前文件位置： 11str = fo.read(11)print(&quot;读取的字符串2：&quot;, str)# result: 读取的字符串2： om www.cctvpostion = fo.seek(0, 0)str = fo.read(11)print(&quot;读取的字符串3：&quot;, str)# result: 读取的字符串3： www.baidu.cfo.close()</code></pre><p>文件重命名</p><pre><code># 文件重命名 filejiatao.txt——&gt;filejiatao2.txtimport ossrc_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;dst_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao2.txt&quot;os.rename(src_file, dst_file)</code></pre><p>删除文件</p><pre><code># 删除一个文件import osdirty_file = r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao2.txt&quot;os.remove(dirty_file)</code></pre><p> 异常处理1</p><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-71be88200641b051.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/627" alt="img"></p><p>2-1.jpg</p><pre><code># 异常处理1try:    fh = open(&quot;testfile.txt&quot;, &quot;w&quot;)    fh.write(&quot;this is my test file for exception handing!&quot;)except IOError:    print(&quot;Eorror:can\&#39;t find file or read data&quot;)else:    print(&quot;witten content in the file successfully&quot;)    fh.close()</code></pre><p>异常处理2</p><pre><code># 异常处理2try:    fh = open(&quot;testfile.txt&quot;, &quot;w&quot;)    fh.write(&quot;this is my test file for exception handing!&quot;)finally:    print(&quot;Eorror:I don\&#39;t kown why ...&quot;)</code></pre><p>异常处理3</p><pre><code># 异常处理3def temp_convert(var):    try:        return int(var)    # except ValueError,Argument:    #     print(&quot;The argument does not contain numbers\n&quot;,Argument)    except (ValueError) as Argument:        print(&quot;The argument does not contain numbers\n&quot;, Argument)temp_convert(&quot;xyz&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python网络爬虫实战之一：网络爬虫理论基础</title>
      <link href="/2019/05/11/python-crawler-basic-internet-1/"/>
      <url>/2019/05/11/python-crawler-basic-internet-1/</url>
      
        <content type="html"><![CDATA[<h1 id="一、浏览网页的基本过程和通信基础"><a href="#一、浏览网页的基本过程和通信基础" class="headerlink" title="一、浏览网页的基本过程和通信基础"></a>一、浏览网页的基本过程和通信基础</h1><blockquote><p>当我们在浏览器地址栏输入： <a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a> 回车后会浏览器显示百度的首页，那这 段网络通信过程中到底发生了什么？</p></blockquote><p>简单来说这段过程发生了以下四个步骤：</p><ol><li>浏览器通过DNS服务器查找域名对应的IP地址;</li><li>向IP地址对应的Web服务器发送请求 ;</li><li>Web Web服务器响应请求，发回HTML页面 ;</li><li>浏览器解析HTML内容，并显示出来</li></ol><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><ul><li>DNS 是计算机域名系统 (Domain Name System (Domain Name System 或 Domain Name Service) 的缩写，由解析器和域名服务组成的。</li><li>域名服务器是指保存有该网络中所主机的和对应 IP 地址，并具有将域名转换为 IP 地址功能的服务器。</li><li>一般个域名的DNS解析时间在10~60毫秒之间。</li><li>一个域名必须对应IP地址，而一个IP地址不一定会有域名</li></ul><h3 id="HTTP和HTTPS"><a href="#HTTP和HTTPS" class="headerlink" title="HTTP和HTTPS"></a>HTTP和HTTPS</h3><ul><li>HTTP协议（HyperText Transfer Protocol，超文本传输协议）是一种发布和接收HTML页面的方法</li><li>HTTPS协议（HyperText Transfer Protocol over Secure Socket Layer）简单的讲就是HTTP的安全版，在HTTP下加入SSL层</li><li>SSL（Secure Socket Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全</li><li>HTTP的端口号是80，HTTPS的端口号是443</li></ul><h3 id="URI与URL"><a href="#URI与URL" class="headerlink" title="URI与URL"></a>URI与URL</h3><ul><li>URI（Uniform Resource Identifier）统一资源标志符</li><li>URL（Universal Resource Locator）统一资源定位符，用于完整地描述Internet上网页和其他资源的地址的一中标识方法</li><li>URL的基本格式：<a href="scheme://host[:port]/path/.../[?query-string][#anchor]" target="_blank" rel="noopener">scheme://host[:port]/path/.../[?query-string][#anchor]</a> </li></ul><h3 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h3><p>请求由客户端向服务端发出，分为四部分：请求方法、请求的网址、请求头、请求体</p><ul><li>请求方法</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-fe7eace52135769d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/751" alt="img"></p><p>  1-1.jpg</p><ul><li>请求头，用来说明服务器使用的附加信息，比较重要的信息有Cookie、Referer、user-Agent等。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-76c4af1982a66838.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/773" alt="img"></p><p>  1-2.jpg</p><ul><li>请求体，一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-569c648c2c6d1f89.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/819" alt="img"></p><p>  1-3.jpg</p><h3 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h3><p>响应由服务端返回给客户端，分为三部分：响应状态码、响应头、响应提</p><ul><li>响应状态码</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-5804a72cba62b6fd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>  1-4.jpg</p><ul><li>响应头，包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/2255795-a61e4fb72096b27e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="img"></p><p>  1-4.jpg</p><ul><li>响应体，响应的正文数据都在响应体中，比如请求网页时他的响应体就是网页的HTML代码，请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的就是响应体</li></ul><h1 id="二、爬虫基本工作原理"><a href="#二、爬虫基本工作原理" class="headerlink" title="二、爬虫基本工作原理"></a>二、爬虫基本工作原理</h1><h3 id="爬虫基本类型"><a href="#爬虫基本类型" class="headerlink" title="爬虫基本类型"></a>爬虫基本类型</h3><ul><li>通用爬虫：是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</li><li>聚焦爬虫：是”面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于， 聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。</li><li>增量式爬虫：增量式更新指的是在更新的时候只更新改变的地方，而未改变的地方则不更新。所以增量式爬虫技术在爬取网页的过程中，只爬取内容发生变化或是新产生的网页，对未发生内容变化的网页则不会爬取。</li></ul><h3 id="爬虫的基本工作流程（以通用爬虫为例）"><a href="#爬虫的基本工作流程（以通用爬虫为例）" class="headerlink" title="爬虫的基本工作流程（以通用爬虫为例）"></a>爬虫的基本工作流程（以通用爬虫为例）</h3><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-cffe88e6b8b23391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/265" alt="img"></p><p>​      1-7.png</p><h5 id="第一步：抓取网页"><a href="#第一步：抓取网页" class="headerlink" title="第一步：抓取网页"></a>第一步：抓取网页</h5><ul><li>首先选取一部分的种子URL，将这些URL放入待抓取URL队列；</li><li>取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。</li><li>分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环</li></ul><h5 id="第二步：数据存储"><a href="#第二步：数据存储" class="headerlink" title="第二步：数据存储"></a>第二步：数据存储</h5><ul><li>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。</li><li>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。</li></ul><h5 id="第三步：预处理"><a href="#第三步：预处理" class="headerlink" title="第三步：预处理"></a>第三步：预处理</h5><ul><li>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理，比如：提取文字、中文分词、消除噪音（比如版权声明文字、导航条、广告等……）、索引处理、链接关系计算、特殊文件处理….</li><li>除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。</li><li>但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。</li></ul><h5 id="第四步：操作数据，实现需求"><a href="#第四步：操作数据，实现需求" class="headerlink" title="第四步：操作数据，实现需求"></a>第四步：操作数据，实现需求</h5><p>比如获取京东某类商品的所有评论、购买用户的会员等级</p><h3 id="爬虫基本结构"><a href="#爬虫基本结构" class="headerlink" title="爬虫基本结构"></a>爬虫基本结构</h3><p><img src="https:////upload-images.jianshu.io/upload_images/2255795-f1dc7e8061ac95ed.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/813" alt="img"></p><p>1-6.jpg</p><h3 id="爬虫的抓取策略"><a href="#爬虫的抓取策略" class="headerlink" title="爬虫的抓取策略"></a>爬虫的抓取策略</h3><ul><li>深度优先遍历策略</li><li>广度优先遍历策略</li><li>方向链接策略</li><li>Partial PageRank 策略</li><li>OPIC策略</li></ul><h3 id="爬虫的更新策略"><a href="#爬虫的更新策略" class="headerlink" title="爬虫的更新策略"></a>爬虫的更新策略</h3><ul><li>历史参考策略</li><li>用户体验策略</li><li>聚类抽样策略</li></ul><h3 id="网页分析算法"><a href="#网页分析算法" class="headerlink" title="网页分析算法"></a>网页分析算法</h3><ul><li>基于用户行为的网页分析算法</li><li>基于网络拓扑的网页分析算法</li><li>基于网页内容的网页分析算法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
