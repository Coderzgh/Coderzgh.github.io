<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python网络爬虫实战之三：基本工具库urllib和requests</title>
      <link href="/2019/05/13/python-crawler-urllib-requests-3/"/>
      <url>/2019/05/13/python-crawler-urllib-requests-3/</url>
      
        <content type="html"><![CDATA[<h1 id="Python网络爬虫实战之三：基本工具库urllib和requests"><a href="#Python网络爬虫实战之三：基本工具库urllib和requests" class="headerlink" title="Python网络爬虫实战之三：基本工具库urllib和requests"></a>Python网络爬虫实战之三：基本工具库urllib和requests</h1><h1 id="一、urllib"><a href="#一、urllib" class="headerlink" title="一、urllib"></a>一、urllib</h1><h3 id="urllib简介"><a href="#urllib简介" class="headerlink" title="urllib简介"></a>urllib简介</h3><p>urllib是Python中一个功能强大用于操作URL，并在爬虫时经常用到的一个基础库，无需额外安装，默认已经安装到python中。</p><h3 id="urllib在python2-x与python3-x中的区别"><a href="#urllib在python2-x与python3-x中的区别" class="headerlink" title="urllib在python2.x与python3.x中的区别"></a>urllib在python2.x与python3.x中的区别</h3><p>在python2.x中，urllib分为urllib和urllib2，在python3.x中合并为urllib。两者使用起来不太一样，注意转换。</p><table><thead><tr><th>Python2.x</th><th>Python3.x</th></tr></thead><tbody><tr><td>import urllib2</td><td>import urllib.request，urllib.error</td></tr><tr><td>import urllib</td><td>import urllib.request，urllib.error，urllib.parse</td></tr><tr><td>import urlparse</td><td>import urllib.parse</td></tr><tr><td>import urlopen</td><td>import urllib.request.urlopen</td></tr><tr><td>import urlencode</td><td>import urllib.parse.urlencode</td></tr><tr><td>import urllib.quote</td><td>import urllib.request.quote</td></tr><tr><td>cookielib.CookieJar</td><td>http.CookieJar</td></tr><tr><td>urllib2.Request</td><td>urllib.request.Request</td></tr></tbody></table><h3 id="urllib的四个子模块"><a href="#urllib的四个子模块" class="headerlink" title="urllib的四个子模块"></a>urllib的四个子模块</h3><p>Python3.6.0中urllib模块包括一下四个子模块，urllib模块是一个运用于URL的包（urllib is a package that collects several modules for working with URLs）</p><ul><li>urllib.request用于访问和读取URLS（urllib.request for opening and reading URLs），就像在浏览器里输入网址然后回车一样，只需要给这个库方法传入URL和其他参数就可以模拟实现这个过程。</li><li>urllib.error包括了所有urllib.request导致的异常（urllib.error containing the exceptions raised by urllib.request），我们可以捕捉这些异常，然后进行重试或者其他操作以确保程序不会意外终止。</li><li>urllib.parse用于解析URLS（urllib.parse for parsing URLs），提供了很多URL处理方法，比如拆分、解析、合并、编码。</li><li>urllib.robotparser用于解析robots.txt文件（urllib.robotparser for parsing robots.txt files），然后判断哪些网站可以爬，哪些网站不可以爬。</li></ul><h3 id="使用urllib打开网页"><a href="#使用urllib打开网页" class="headerlink" title="使用urllib打开网页"></a>使用urllib打开网页</h3><p>最基本的方法打开网页</p><pre><code># 最基本的方法打开网页from urllib.request import urlopenresponse = urlopen(&quot;http://www.baidu.com&quot;)print(type(response))print(response.status)print(response.getheaders())print(response.getheader(&#39;Server&#39;))html = response.read()print(html)</code></pre><p>携带data参数打开网页</p><pre><code># 携带data参数打开网页from urllib.parse import urlencodefrom urllib.request import urlopendata = bytes(urlencode({&#39;word&#39;: &#39;hello&#39;}), encoding=&#39;utf8&#39;)response = urlopen(&#39;http://httpbin.org/post&#39;, data=data)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>携带timeout参数打开网页1</p><pre><code>#  携带timeout参数打开网页1from urllib.request import urlopen# response = urllib.request.urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=1)print(response.read())</code></pre><p>携带timeout参数打开网页2</p><pre><code># 携带timeout参数打开网页2from urllib.request import urlopentry:    response = urlopen(&#39;http://httpbin.org/get&#39;, timeout=0.1)    print(response.read())except Exception as e:    print(e)</code></pre><p>通过构建Request打开网页1</p><pre><code># 通过构建Request打开网页1from urllib.request import Requestfrom urllib.request import urlopenrequest = Request(&#39;https://python.org&#39;)response = urlopen(request)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页2</p><pre><code># 通过构建Request打开网页2from urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;headers = {    &#39;User-Agent&#39;: &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;,    &#39;Host&#39;: &#39;httpbin.org&#39;}dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, headers=headers, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>与通过构建Request打开网页2对比</p><pre><code># 与通过构建Request打开网页2对比from urllib.request import Requestfrom urllib.request import urlopenreq = Request(url=url, data=data, method=&#39;POST&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>通过构建Request打开网页3：通过add_header方</p><pre><code># 通过构建Request打开网页3：通过add_header方法添加headersfrom urllib.request import Requestfrom urllib.request import urlopenfrom urllib.parse import urlencodeurl = &#39;http://httpbin.org/post&#39;dict = {&#39;name&#39;: &#39;Germey&#39;}data = bytes(urlencode(dict), encoding=&#39;utf8&#39;)req = Request(url=url, data=data, method=&#39;POST&#39;)req.add_header(&#39;User-Agent&#39;, &#39;Mozilla/4.0(compatibe;MSIE 5.5;Windows NT)&#39;)response = urlopen(req)print(response.read().decode(&#39;utf-8&#39;))</code></pre><p>urlencode()的使用</p><pre><code># urlencode()的使用from urllib.parse import urlencodefrom urllib.request import urlopendata = {&#39;first&#39;: &#39;true&#39;, &#39;pn&#39;: 1, &#39;kd&#39;: &#39;Python&#39;}data = urlencode(data).encode(&#39;utf-8&#39;)datapage = urlopen(req, data=data).read()page</code></pre><p>使用代理打开网页</p><pre><code># 使用代理from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy_handler = ProxyHandler({&#39;http&#39;: &#39;106.56.102.140:8070&#39;})opener = build_opener(proxy_handler)try:    response = opener.open(&#39;http://www.baidu.com/&#39;)    print(response.read().decode(&#39;utf-8&#39;))except URLError as e:    print(e.reason)</code></pre><h1 id="二、requests"><a href="#二、requests" class="headerlink" title="二、requests"></a>二、requests</h1><p>相比较urllib模块，requests模块要简单很多，但是需要单独安装：</p><ul><li>在windows系统下只需要在命令行输入命令 pip install requests 即可安装。</li><li>在 linux 系统下，只需要输入命令 sudo pip install requests ，即可安装。</li></ul><p>requests库的八个主要方法</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>requests.request()</td><td>构造一个请求，支持以下各种方法</td></tr><tr><td>requests.get()</td><td>向html网页提交get请求的方法</td></tr><tr><td>requests.post()</td><td>向html网页提交post请求的方法</td></tr><tr><td>requests.head()</td><td>获取html头部信息的主要方法</td></tr><tr><td>requests.put()</td><td>向html网页提交put请求的方法</td></tr><tr><td>requests.options()</td><td>向html网页提交options请求的方法</td></tr><tr><td>requests.patch()</td><td>向html网页提交局部修改的请求</td></tr><tr><td>requests.delete()</td><td>向html网页提交删除的请求</td></tr></tbody></table><p>请求之后，服务器通过response返回数据，response具体参数如下图：</p><table><thead><tr><th>属性</th><th>描述</th></tr></thead><tbody><tr><td>r.status_code</td><td>http请求的返回状态，若为200则表示请求成功</td></tr><tr><td>r.text</td><td>http响应内容的字符串形式，即返回的页面内容</td></tr><tr><td>r.encoding</td><td>从http header 中猜测的相应内容编码方式</td></tr><tr><td>r.apparent_encoding</td><td>从内容中分析出的响应内容编码方式（备选编码方式）</td></tr><tr><td>r.content</td><td>http响应内容的二进制形式</td></tr></tbody></table><h3 id="requests-request-method-url-kwargs"><a href="#requests-request-method-url-kwargs" class="headerlink" title="requests.request(method, url, **kwargs)"></a>requests.request(method, url, **kwargs)</h3><ul><li>method：即 get、post、head、put、options、patch、delete</li><li>url：即请求的网址</li><li>**kwargs：控制访问的参数，具体参数如下：</li><li><ul><li>params：字典或字节序列，作为参数增加到url中。使用这个参数可以把一些键值对以?key1=value1&amp;key2=value2的模式增加到url中</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 paramsimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>data：字典，字节序或文件对象，重点作为向服务器提供或提交资源是提交，作为request的内容，与params不同的是，data提交的数据并不放在url链接里， 而是放在url链接对应位置的地方作为数据来存储。它也可以接受一个字符串对象。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 dataimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, data=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;23&quot;, #     &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>json：json格式的数据， json合适在相关的html，http相关的web开发中非常常见， 也是http最经常使用的数据格式， 他是作为内容部分可以向服务器提交。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 jsonimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, json=payload)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;{\&quot;key1\&quot;: \&quot;value1\&quot;, \&quot;key2\&quot;: \&quot;value2\&quot;}&quot;, #   &quot;files&quot;: {}, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;36&quot;, #     &quot;Content-Type&quot;: &quot;application/json&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>files：字典， 是用来向服务器传输文件时使用的字段。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 fileimport requests# filejiatao.txt 文件的内容是文本“www.baidu.com www.cctvjiatao.com”files = {&#39;file&#39;: open(r&quot;D:\DataguruPyhton\PythonSpider\lesson2\filejiatao.txt&quot;, &quot;rb&quot;)}r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, files=files)print(r.url)# result: http://httpbin.org/postprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;data&quot;: &quot;&quot;, #   &quot;files&quot;: {#     &quot;file&quot;: &quot;www.baidu.com www.cctvjiatao.com&quot;#   }, #   &quot;form&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Content-Length&quot;: &quot;182&quot;, #     &quot;Content-Type&quot;: &quot;multipart/form-data; boundary=ee12ea6a4fd2b8a3318566775f2b268f&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;json&quot;: null, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/post&quot;# }</code></pre><ul><li><ul><li>headers：字典是http的相关语，对应了向某个url访问时所发起的http的头字段， 可以用这个字段来定义http的访问的http头，可以用来模拟任何我们想模拟的浏览器来对url发起访问。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 headersimport requestspayload = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=payload, headers=headers)print(r.url)# result: http://httpbin.org/get?key1=value1&amp;key2=value2print(r.text)# result:# {#   &quot;args&quot;: {#     &quot;key1&quot;: &quot;value1&quot;, #     &quot;key2&quot;: &quot;value2&quot;#   }, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get?key1=value1&amp;key2=value2&quot;# }</code></pre><ul><li><ul><li>cookies：字典或CookieJar，指的是从http中解析cookie</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 cookiesimport requestscookies = dict(cookies_are=&#39;working&#39;)r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/cookies&#39;, cookies=cookies)print(r.url)# result: http://httpbin.org/cookiesprint(r.text)# result:# {#   &quot;cookies&quot;: {#     &quot;cookies_are&quot;: &quot;working&quot;#   }# }</code></pre><ul><li><ul><li>auth：元组，用来支持http认证功能</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 authimport requestscs_user = &#39;用户名&#39;cs_psw = &#39;密码&#39;r = requests.request(&#39;GET&#39;, &#39;https://api.github.com&#39;, auth=(cs_user, cs_psw))print(r.url)# result: 待补充print(r.text)# result: 待补充</code></pre><ul><li><ul><li>timeout: 用于设定超时时间， 单位为秒，当发起一个get请求时可以设置一个timeout时间， 如果在timeout时间内请求内容没有返回， 将产生一个timeout的异常。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 timeoutimport requestsr = requests.request(&#39;GET&#39;, &#39;http://github.com&#39;, timeout=0.001)print(r.url)# result: 报错 socket.timeout: timed out</code></pre><ul><li><ul><li>proxies：字典， 用来设置访问代理服务器。</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 proxiesimport requestsproxies = {    &#39;https&#39;: &#39;http://41.118.132.69:4433&#39;}# 也可以通过环境变量设置代理# export HTTP_PROXY=&#39;http://10.10.1.10:3128&#39;# export HTTPS_PROXY=&#39;http://10.10.1.10:1080&#39;r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, proxies=proxies)print(r.url)# result: http://httpbin.org/getprint(r.text)# result:# {#   &quot;args&quot;: {}, #   &quot;headers&quot;: {#     &quot;Accept&quot;: &quot;*/*&quot;, #     &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, #     &quot;Connection&quot;: &quot;close&quot;, #     &quot;Host&quot;: &quot;httpbin.org&quot;, #     &quot;User-Agent&quot;: &quot;python-requests/2.19.1&quot;#   }, #   &quot;origin&quot;: &quot;1.203.183.95&quot;, #   &quot;url&quot;: &quot;http://httpbin.org/get&quot;# }</code></pre><ul><li><ul><li>verify：开关， 用于认证SSL证书， 默认为True</li></ul></li></ul><pre><code>## request(method, url, **kwargs)，当 **kwargs 为 verify，SSL证书验证import requestsr = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=True)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://kyfw.12306.cn/otn/&#39;, verify=False)print(r.text)r = requests.request(&#39;GET&#39;, &#39;https://github.com&#39;, verify=True)print(r.text)</code></pre><ul><li><ul><li>allow_redirects: 开关， 表示是否允许对url进行重定向， 默认为True。</li></ul></li><li><ul><li>stream: 开关， 指是否对获取内容进行立即下载， 默认为True。</li></ul></li><li><ul><li>cert： 用于设置保存本地SSL证书路径</li></ul></li></ul><h3 id="requests-get-url-params-None-kwargs"><a href="#requests-get-url-params-None-kwargs" class="headerlink" title="requests.get(url, params=None, **kwargs)"></a>requests.get(url, params=None, **kwargs)</h3><pre><code># 官方文档def get(url, params=None, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;get&#39;, url, params=params, **kwargs)</code></pre><h3 id="requests-post-url-data-None-json-None-kwargs"><a href="#requests-post-url-data-None-json-None-kwargs" class="headerlink" title="requests.post(url, data=None, json=None, **kwargs)"></a>requests.post(url, data=None, json=None, **kwargs)</h3><pre><code># 官方文档def post(url, data=None, json=None, **kwargs):    return request(&#39;post&#39;, url, data=data, json=json, **kwargs)</code></pre><h3 id="requests-head-url-kwargs"><a href="#requests-head-url-kwargs" class="headerlink" title="requests.head(url, **kwargs)"></a>requests.head(url, **kwargs)</h3><pre><code># 官方文档def head(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, False)    return request(&#39;head&#39;, url, **kwargs)</code></pre><h3 id="requests-options-url-kwargs"><a href="#requests-options-url-kwargs" class="headerlink" title="requests.options(url, **kwargs)"></a>requests.options(url, **kwargs)</h3><pre><code># 官方文档def options(url, **kwargs):    kwargs.setdefault(&#39;allow_redirects&#39;, True)    return request(&#39;options&#39;, url, **kwargs)</code></pre><h3 id="requests-put-url-data-None-kwargs"><a href="#requests-put-url-data-None-kwargs" class="headerlink" title="requests.put(url, data=None, **kwargs)"></a>requests.put(url, data=None, **kwargs)</h3><pre><code># 官方文档def put(url, data=None, **kwargs):    return request(&#39;put&#39;, url, data=data, **kwargs)</code></pre><h3 id="requests-patch-url-data-None-kwargs"><a href="#requests-patch-url-data-None-kwargs" class="headerlink" title="requests.patch(url, data=None, **kwargs)"></a>requests.patch(url, data=None, **kwargs)</h3><pre><code># 官方文档def patch(url, data=None, **kwargs):    return request(&#39;patch&#39;, url, data=data, **kwargs)</code></pre><blockquote><p>requests.patch和request.put类似。<br> 两者不同的是： 当我们用patch时仅需要提交需要修改的字段。<br> 而用put时，必须将所有字段一起提交到url，未提交字段将会被删除。<br> patch的好处是：节省网络带宽。</p></blockquote><h3 id="requests-delete-url-kwargs"><a href="#requests-delete-url-kwargs" class="headerlink" title="requests.delete(url, **kwargs)"></a>requests.delete(url, **kwargs)</h3><pre><code># 官方文档def delete(url, **kwargs):    return request(&#39;delete&#39;, url, **kwargs)</code></pre><h3 id="requests库的异常"><a href="#requests库的异常" class="headerlink" title="requests库的异常"></a>requests库的异常</h3><p>注意requests库有时会产生异常，比如网络连接错误、http错误异常、重定向异常、请求url超时异常等等。所以我们需要判断r.status_codes是否是200，在这里我们怎么样去捕捉异常呢？<br> 这里我们可以利用r.raise_for_status() 语句去捕捉异常，该语句在方法内部判断r.status_code是否等于200，如果不等于，则抛出异常。<br> 于是在这里我们有一个爬取网页的通用代码框架</p><pre><code>try:    r = requests.get(url, timeout=30)  # 请求超时时间为30秒    r.raise_for_status()  # 如果状态不是200，则引发异常    r.encoding = r.apparent_encoding  # 配置编码    print(r.text)except:    print(&quot;产生异常&quot;)</code></pre><h1 id="三、requests的综合小实例"><a href="#三、requests的综合小实例" class="headerlink" title="三、requests的综合小实例"></a>三、requests的综合小实例</h1><h3 id="实例一：京东商品信息的爬取"><a href="#实例一：京东商品信息的爬取" class="headerlink" title="实例一：京东商品信息的爬取"></a>实例一：京东商品信息的爬取</h3><pre><code>## 京东商品信息的爬取# 不需要对头部做任何修改，即可爬网页import requestsurl = &#39;http://item.jd.com/2967929.html&#39;try:    r = requests.get(url, timeout=30)    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[:1000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例二：亚马逊商品信息的爬取"><a href="#实例二：亚马逊商品信息的爬取" class="headerlink" title="实例二：亚马逊商品信息的爬取"></a>实例二：亚马逊商品信息的爬取</h3><pre><code>## 亚马逊商品信息的爬取# 该网页中对爬虫进行的爬取做了限制，因此我们需要伪装自己为浏览器发出的请求import requestsurl = &#39;http://www.amazon.cn/gp/product/B01M8L5Z3Y&#39;try:    kv = {&#39;user_agent&#39;: &#39;Mozilla/5.0&#39;}    r = requests.get(url, headers=kv)  # 改变自己的请求数据    r.raise_for_status()    r.encoding = r.apparent_encoding    print(r.text[1000:2000])  # 部分信息except:    print(&quot;失败&quot;)</code></pre><h3 id="实例三：百度搜索关键字提交"><a href="#实例三：百度搜索关键字提交" class="headerlink" title="实例三：百度搜索关键字提交"></a>实例三：百度搜索关键字提交</h3><pre><code>## 百度搜索关键字提交# 百度的关键字接口：https://www.baidu.com/s?wd=keywordimport requestskeyword = &#39;python&#39;try:    kv = {&#39;wd&#39;: keyword}    headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&quot;}    r = requests.get(&#39;https://www.baidu.com/s&#39;, params=kv, headers=headers)    r.raise_for_status()    r.encoding = r.apparent_encoding    # print(len(r.text))    print(r.text)except:    print(&quot;失败&quot;)</code></pre><h3 id="实例四：网络图片的爬取"><a href="#实例四：网络图片的爬取" class="headerlink" title="实例四：网络图片的爬取"></a>实例四：网络图片的爬取</h3><pre><code>## 网络图片的爬取import requestsimport ostry:    url = &quot;https://odonohz90.qnssl.com/library/145456/bb0b3faa7a872d012bb4c57256b47585.jpg?imageView2/2/w/1000/h/1000/q/75&quot;  # 图片地址    root = r&quot;D:\DataguruPyhton\PythonSpider\lesson3\pic\\&quot;    path = root + url.split(&quot;/&quot;)[-1]    if not path.endswith(&quot;.jpg&quot;):        path += &quot;.jpg&quot;    if not os.path.exists(root):  # 目录不存在创建目录        os.mkdir(root)    if not os.path.exists(path):  # 文件不存在则下载        r = requests.get(url)        f = open(path, &quot;wb&quot;)        f.write(r.content)        f.close()        print(&quot;文件下载成功&quot;)    else:        print(&quot;文件已经存在&quot;)except:    print(&quot;获取失败&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> requests </tag>
            
            <tag> urllib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文章title</title>
      <link href="/2019/05/06/title/"/>
      <url>/2019/05/06/title/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>重新更新这个博客-2018-12-04</title>
      <link href="/2019/05/05/hello-world/"/>
      <url>/2019/05/05/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
